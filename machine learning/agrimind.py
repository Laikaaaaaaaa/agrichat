"""agrimind.py

AgriMind (AgriChat) - Hybrid NLP + KB + Safety Rules

Goals (practical MVP):
- Multi-label entity extraction (specie/season/disease/symptoms) directly from user question.
- Safe KB matching + rule engine to avoid hallucinating new info.
- Prompt generator (app-style: friendly Vietnamese + icons) to send to LLM later.
- CLI + REST API with caching.

Notes:
- Runs without heavy ML dependencies by default.
- Optional: Transformers / Torch can be plugged in later.
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
import time
import unicodedata
from dataclasses import dataclass
from functools import lru_cache
from typing import Any, Dict, List, Optional, Tuple


HERE = os.path.dirname(os.path.abspath(__file__))
DEFAULT_DATASET_PATH = os.path.join(HERE, "dataset", "dataset.json")


def _normalize(text: str) -> str:
    if not text:
        return ""
    text = text.strip().lower()
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    text = re.sub(r"\s+", " ", text)
    return text


@dataclass(frozen=True)
class KBEntry:
    id: str
    domain: str
    specie: str
    season: str
    disease: str
    symptoms: Tuple[str, ...]
    causes: Tuple[str, ...]
    advice: Tuple[str, ...]
    examples: Tuple[str, ...]
    safety_urgent: bool
    safety_notes: Tuple[str, ...]


def load_dataset(path: str = DEFAULT_DATASET_PATH) -> List[KBEntry]:
    with open(path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    entries: List[KBEntry] = []
    for item in raw:
        safety = item.get("safety") or {}
        entries.append(
            KBEntry(
                id=str(item.get("id") or ""),
                domain=str(item.get("domain") or "unknown"),
                specie=str(item.get("specie") or "").strip(),
                season=str(item.get("season") or "").strip(),
                disease=str(item.get("disease") or "").strip(),
                symptoms=tuple(item.get("symptoms") or []),
                causes=tuple(item.get("causes") or []),
                advice=tuple(item.get("advice") or []),
                examples=tuple(item.get("examples") or []),
                safety_urgent=bool(safety.get("urgent", False)),
                safety_notes=tuple(safety.get("notes") or []),
            )
        )

    # Basic validation
    bad = [e for e in entries if not e.specie or not e.season or not e.disease]
    if bad:
        raise ValueError(f"Dataset has {len(bad)} invalid entries missing specie/season/disease")
    return entries


def _build_lexicons(entries: List[KBEntry]) -> Dict[str, Any]:
    species = sorted({e.specie for e in entries})
    seasons = sorted({e.season for e in entries})
    diseases = sorted({e.disease for e in entries})

    all_symptoms: List[str] = []
    for e in entries:
        all_symptoms.extend(list(e.symptoms))
    symptoms = sorted({s for s in all_symptoms if s})

    # Simple aliases (expand as needed)
    season_aliases = {
        "mua": ["mua", "mua mua", "troi mua", "mua lon", "am uot"],
        "nang": ["nang", "troi nang", "kho", "kho han"],
        "bat_ky": ["bat ky", "quanh nam", "luc nao"],
    }

    specie_aliases = {
        "heo": ["heo", "lon"],
        "ga": ["ga", "ga ta", "ga cong nghiep"],
        "bo": ["bo", "bo sua", "bo thit"],
        "tom": ["tom", "tom the", "tom su"],
        "lua": ["lua", "ruong lua"],
        "ca chua": ["ca chua", "tomato"],
        "ot": ["ot", "ot hiem"],
        "xoai": ["xoai", "mango"],
        "cam": ["cam", "quyt", "citrus"],
    }

    return {
        "species": species,
        "seasons": seasons,
        "diseases": diseases,
        "symptoms": symptoms,
        "season_aliases": season_aliases,
        "specie_aliases": specie_aliases,
    }


def _find_first_match(text_norm: str, candidates: List[str]) -> Optional[str]:
    for cand in candidates:
        cand_norm = _normalize(cand)
        if cand_norm and re.search(rf"\b{re.escape(cand_norm)}\b", text_norm):
            return cand
    return None


def _extract_symptoms(text_norm: str, symptom_list: List[str]) -> List[str]:
    found: List[str] = []
    for s in symptom_list:
        s_norm = _normalize(s)
        if not s_norm:
            continue
        # allow partial match (symptoms are often phrases)
        if s_norm in text_norm:
            found.append(s)
    # de-dup preserving order
    dedup: List[str] = []
    seen = set()
    for s in found:
        key = _normalize(s)
        if key not in seen:
            seen.add(key)
            dedup.append(s)
    return dedup


def extract_entities(question: str, entries: List[KBEntry], lex: Dict[str, Any]) -> Dict[str, Any]:
    q_norm = _normalize(question)

    # 1) Specie (dictionary + aliases)
    specie: Optional[str] = None
    for canonical, aliases in lex.get("specie_aliases", {}).items():
        for alias in aliases:
            if _normalize(alias) and _normalize(alias) in q_norm:
                # map back to canonical specie label present in dataset when possible
                for s in lex["species"]:
                    if _normalize(s) == canonical:
                        specie = s
                        break
                specie = specie or canonical
                break
        if specie:
            break
    if not specie:
        specie = _find_first_match(q_norm, lex["species"])

    # 2) Season
    season: Optional[str] = None
    for canonical, aliases in lex.get("season_aliases", {}).items():
        for alias in aliases:
            if _normalize(alias) and _normalize(alias) in q_norm:
                for s in lex["seasons"]:
                    if _normalize(s) == canonical:
                        season = s
                        break
                season = season or canonical
                break
        if season:
            break
    if not season:
        season = _find_first_match(q_norm, [s for s in lex["seasons"] if s != "bat_ky"]) or None

    # 3) Disease
    disease = _find_first_match(q_norm, lex["diseases"])

    # 4) Symptoms
    symptoms_found = _extract_symptoms(q_norm, lex["symptoms"])

    return {
        "question": question,
        "specie": specie,
        "season": season,
        "disease": disease,
        "symptoms": symptoms_found,
    }


def _score_entry(extracted: Dict[str, Any], entry: KBEntry) -> float:
    score = 0.0

    if extracted.get("specie") and _normalize(extracted["specie"]) == _normalize(entry.specie):
        score += 3.0
    if extracted.get("season") and _normalize(extracted["season"]) == _normalize(entry.season):
        score += 2.0
    if extracted.get("disease") and _normalize(extracted["disease"]) == _normalize(entry.disease):
        score += 4.0

    ex_symptoms = extracted.get("symptoms") or []
    if ex_symptoms:
        entry_sym = {_normalize(s) for s in entry.symptoms}
        overlap = sum(1 for s in ex_symptoms if _normalize(s) in entry_sym)
        score += min(3.0, overlap * 1.0)

    # small bias toward urgent if user uses alarming words
    qn = _normalize(extracted.get("question") or "")
    if entry.safety_urgent and any(w in qn for w in ["chet", "kho tho", "ra mau", "soc", "ngat"]):
        score += 0.5
    return score


def match_kb(extracted: Dict[str, Any], entries: List[KBEntry]) -> Tuple[Optional[KBEntry], float]:
    best: Optional[KBEntry] = None
    best_score = -1.0
    for e in entries:
        s = _score_entry(extracted, e)
        if s > best_score:
            best = e
            best_score = s

    # Normalize to [0,1] with a simple cap (heuristic)
    confidence = max(0.0, min(1.0, best_score / 10.0))
    return best, confidence


def rule_engine(extracted: Dict[str, Any], entry: Optional[KBEntry], confidence: float) -> Dict[str, Any]:
    warnings: List[str] = []
    actions: List[str] = []
    allow_answer = True

    # If no KB match or too low confidence -> ask clarifying questions
    if not entry or confidence < 0.35:
        allow_answer = False
        actions.append("ask_clarify")
        warnings.append("Ch∆∞a ƒë·ªß th√¥ng tin ƒë·ªÉ k·∫øt lu·∫≠n. C·∫ßn h·ªèi th√™m lo√†i/m√πa/tri·ªáu ch·ª©ng c·ª• th·ªÉ.")

    # Safety: urgent entries -> recommend contacting specialist
    if entry and entry.safety_urgent:
        warnings.append("D·∫•u hi·ªáu c√≥ th·ªÉ nghi√™m tr·ªçng. N√™n theo d√µi s√°t v√† li√™n h·ªá chuy√™n gia/th√∫ y/k·ªπ thu·∫≠t ƒë·ªãa ph∆∞∆°ng n·∫øu n·∫∑ng.")

    # Never hallucinate: advice must come from KB
    return {
        "allow_answer": allow_answer,
        "warnings": warnings,
        "actions": actions,
    }


def suggest_next_question(extracted: Dict[str, Any], entry: Optional[KBEntry], confidence: float) -> str:
    # Rule-first (works now). LSTM can be added later.
    missing = []
    if not extracted.get("specie"):
        missing.append("lo√†i/c√¢y nu√¥i‚Äìtr·ªìng")
    if not extracted.get("season"):
        missing.append("m√πa/th·ªùi ti·∫øt g·∫ßn ƒë√¢y")
    if not extracted.get("symptoms"):
        missing.append("tri·ªáu ch·ª©ng c·ª• th·ªÉ")
    if missing:
        return f"B·∫°n cho m√¨nh xin th√™m {', '.join(missing)} nh√©?"

    if entry and confidence < 0.65:
        return "B·∫°n cho m√¨nh bi·∫øt t√¨nh tr·∫°ng k√©o d√†i bao l√¢u v√† m·ª©c ƒë·ªô n·∫∑ng/nh·∫π (c√≥ s·ªët, b·ªè ƒÉn, ch·∫øt r·∫£i r√°c kh√¥ng)?"
    return "B·∫°n mu·ªën m√¨nh h∆∞·ªõng d·∫´n ph√≤ng ng·ª´a t√°i ph√°t v√† c√°ch theo d√µi ti·∫øp theo kh√¥ng?"


def _format_list(items: Tuple[str, ...] | List[str], bullet: str = "- ") -> str:
    return "\n".join(f"{bullet}{x}" for x in items if x)


def generate_prompt(question: str, extracted: Dict[str, Any], entry: Optional[KBEntry], confidence: float, rules: Dict[str, Any]) -> str:
    # This prompt is meant to be fed to a generative model later.
    # Keep it strict: use KB only, ask clarifying if uncertain.
    specie = extracted.get("specie") or "(ch∆∞a r√µ)"
    season = extracted.get("season") or "(ch∆∞a r√µ)"
    disease = extracted.get("disease") or "(ch∆∞a r√µ)"
    symptoms = extracted.get("symptoms") or []

    kb_block = ""
    if entry:
        kb_block = (
            "\nüìö D·ªÆ LI·ªÜU THAM CHI·∫æU (Knowledge Base)\n"
            f"- Domain: {entry.domain}\n"
            f"- Lo√†i/C√¢y: {entry.specie}\n"
            f"- M√πa: {entry.season}\n"
            f"- V·∫•n ƒë·ªÅ/B·ªánh: {entry.disease}\n"
            "\nüîé Tri·ªáu ch·ª©ng th∆∞·ªùng g·∫∑p:\n"
            f"{_format_list(entry.symptoms)}\n"
            "\nüß© Nguy√™n nh√¢n c√≥ th·ªÉ:\n"
            f"{_format_list(entry.causes)}\n"
            "\n‚úÖ Khuy·∫øn ngh·ªã an to√†n:\n"
            f"{_format_list(entry.advice)}\n"
        )

    warnings = rules.get("warnings") or []
    warning_block = ""
    if warnings:
        warning_block = "\n‚ö†Ô∏è L∆ØU √ù AN TO√ÄN\n" + "\n".join(f"- {w}" for w in warnings)

    # App-style: friendly, short paragraphs, icons, ask follow-up.
    next_q = suggest_next_question(extracted, entry, confidence)

    prompt = f"""
B·∫°n l√† AgriSense AI üå± ‚Äî tr·ª£ l√Ω n√¥ng nghi·ªáp th√¢n thi·ªán.

Y√äU C·∫¶U TR·∫¢ L·ªúI:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán, d·ªÖ hi·ªÉu.
- Kh√¥ng b·ªãa th√¥ng tin. N·∫øu thi·∫øu d·ªØ li·ªáu th√¨ h·ªèi l·∫°i cho r√µ.
- Ch·ªâ d√πng th√¥ng tin trong KB ph√≠a d∆∞·ªõi khi ƒë∆∞a khuy·∫øn ngh·ªã.
- Lu√¥n nh·∫Øc an to√†n khi c√≥ d·∫•u hi·ªáu nghi√™m tr·ªçng.

üë§ C√ÇU H·ªéI NG∆Ø·ªúI D√ôNG:
{question}

üß† TR√çCH XU·∫§T T·ª™ C√ÇU H·ªéI (∆∞·ªõc l∆∞·ª£ng):
- Lo√†i/C√¢y: {specie}
- M√πa: {season}
- B·ªánh/V·∫•n ƒë·ªÅ: {disease}
- Tri·ªáu ch·ª©ng: {', '.join(symptoms) if symptoms else '(ch∆∞a r√µ)'}
- ƒê·ªô tin c·∫≠y kh·ªõp KB: {confidence:.2f}
{kb_block}
{warning_block}

‚úÖ H√£y tr·∫£ l·ªùi theo c·∫•u tr√∫c:
1) Nh·∫≠n ƒë·ªãnh ng·∫Øn g·ªçn (1‚Äì2 c√¢u)
2) Vi·ªác c·∫ßn l√†m ngay (bullet)
3) Theo d√µi th√™m d·∫•u hi·ªáu n√†o
4) C√¢u h·ªèi ti·∫øp theo ƒë·ªÉ ch·∫©n ƒëo√°n ch√≠nh x√°c h∆°n: {next_q}
""".strip()

    return prompt


@lru_cache(maxsize=1024)
def _cached_extract(question: str, dataset_path: str) -> Dict[str, Any]:
    entries = load_dataset(dataset_path)
    lex = _build_lexicons(entries)
    extracted = extract_entities(question, entries, lex)
    entry, confidence = match_kb(extracted, entries)
    rules = rule_engine(extracted, entry, confidence)
    return {
        "extracted": extracted,
        "matched": (entry.__dict__ if entry else None),
        "confidence": confidence,
        "rules": rules,
    }


@lru_cache(maxsize=512)
def _cached_prompt(question: str, dataset_path: str) -> str:
    result = _cached_extract(question, dataset_path)
    extracted = result["extracted"]
    entry_raw = result["matched"]
    entry = KBEntry(**entry_raw) if entry_raw else None
    confidence = float(result["confidence"])
    rules = result["rules"]
    return generate_prompt(question, extracted, entry, confidence, rules)


def cli_extract(args: argparse.Namespace) -> int:
    out = _cached_extract(args.text, args.dataset)
    print(json.dumps(out, ensure_ascii=False, indent=2))
    return 0


def cli_prompt(args: argparse.Namespace) -> int:
    prompt = _cached_prompt(args.text, args.dataset)
    print(prompt)
    return 0


def cli_serve(args: argparse.Namespace) -> int:
    from flask import Flask, jsonify, request

    app = Flask(__name__)

    @app.get("/health")
    def health():
        return jsonify({"ok": True, "service": "agrimind", "ts": time.time()})

    @app.post("/agrimind/extract")
    def api_extract():
        payload = request.get_json(silent=True) or {}
        text = str(payload.get("text") or "").strip()
        if not text:
            return jsonify({"success": False, "error": "text is required"}), 400
        return jsonify({"success": True, "result": _cached_extract(text, args.dataset)})

    @app.post("/agrimind/prompt")
    def api_prompt():
        payload = request.get_json(silent=True) or {}
        text = str(payload.get("text") or "").strip()
        if not text:
            return jsonify({"success": False, "error": "text is required"}), 400
        return jsonify({"success": True, "prompt": _cached_prompt(text, args.dataset)})

    app.run(host=args.host, port=args.port, debug=False)
    return 0


def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="AgriMind - entity extraction + KB + safety + prompt generator")
    p.add_argument("--dataset", default=DEFAULT_DATASET_PATH, help="Path to dataset.json")
    sub = p.add_subparsers(dest="cmd", required=True)

    p_extract = sub.add_parser("extract", help="Extract entities + match KB")
    p_extract.add_argument("text", help="User question")
    p_extract.set_defaults(func=cli_extract)

    p_prompt = sub.add_parser("prompt", help="Generate LLM prompt (app-style)")
    p_prompt.add_argument("text", help="User question")
    p_prompt.set_defaults(func=cli_prompt)

    p_serve = sub.add_parser("serve", help="Run REST API (Flask)")
    p_serve.add_argument("--host", default="127.0.0.1")
    p_serve.add_argument("--port", type=int, default=8011)
    p_serve.set_defaults(func=cli_serve)

    return p


def main() -> int:
    # Windows consoles may default to legacy codepages; this avoids UnicodeEncodeError
    # when printing Vietnamese text.
    try:
        sys.stdout.reconfigure(encoding="utf-8")
        sys.stderr.reconfigure(encoding="utf-8")
    except Exception:
        pass

    parser = build_argparser()
    args = parser.parse_args()
    return int(args.func(args))


if __name__ == "__main__":
    raise SystemExit(main())
