"""clarify_intent.py

ML + rule-first detector for unclear/underspecified agriculture questions.

Goal:
- Detect when a user's question is too vague to answer confidently (missing key details)
- Ask for the missing details using a friendly clarification prompt
- Avoid calling LLM APIs for these cases (optional integration in app.py)

Datasets:
- dataset/clarify_intent.json   (text,label)  label 0 = unclear, 1 = clear
- dataset/clarify_replies.json  (list of clarification replies)

Model:
- model/clarify_intent.pkl

Env vars:
- CLARIFY_INTENT_MODEL_SOURCE=auto|off|local (default: auto)
- CLARIFY_INTENT_THRESHOLD=0.65             (default: 0.65)  # threshold on P(unclear)
- CLARIFY_REPLY_MODE=sample|mixed           (default: sample)
"""

from __future__ import annotations

import argparse
import json
import os
import pickle
import random
import re
import unicodedata
from functools import lru_cache
from typing import Any, Dict, Iterable, List, Optional, Tuple


HERE = os.path.dirname(os.path.abspath(__file__))
DEFAULT_DATASET_PATH = os.path.join(HERE, "dataset", "clarify_intent.json")
DEFAULT_REPLIES_PATH = os.path.join(HERE, "dataset", "clarify_replies.json")
DEFAULT_MODEL_DIR = os.path.join(HERE, "model")
DEFAULT_MODEL_PATH = os.path.join(DEFAULT_MODEL_DIR, "clarify_intent.pkl")


def _normalize(text: str) -> str:
    if not text:
        return ""
    text = text.strip().lower()
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    text = re.sub(r"\s+", " ", text)
    return text


def _tokenize(text_norm: str) -> List[str]:
    if not text_norm:
        return []
    parts = re.split(r"[^\w]+", text_norm)
    return [p for p in parts if p]


# High-level agriculture hints
_AGRI_HINT_TOKENS = {
    # crop
    "trong",
    "gieo",
    "vuon",
    "ruong",
    "cay",
    "la",
    "re",
    "than",
    "hoa",
    "trai",
    "hat",
    "phan",
    "bon",
    "npk",
    "thuoc",
    "sau",
    "benh",
    "nam",
    "dao",
    "on",
    # livestock
    "heo",
    "lon",
    "ga",
    "vit",
    "bo",
    "de",
    "chuong",
    # aquaculture
    "tom",
    "ca",
    "ao",
    "be",
    "nuoc",
    "ph",
    "kiem",
    "do",
}


# Patterns that often indicate a generic request (likely needs clarification)
_GENERIC_PATTERNS = [
    r"\btu van\b",
    r"\bhuong dan\b",
    r"\bchi\s*dan\b",
    r"\bcach\b",
    r"\bphong\s*benh\b",
    r"\btri\s*benh\b",
    r"\bthuoc\b",
    r"\bphan\b",
    r"\bkhau\s*phan\b",
    r"\bcong\s*thuc\b",
]


_DETAIL_HINT_TOKENS = {
    # time / stage
    "ngay",
    "tuan",
    "thang",
    "nam",
    "giai",
    "doan",
    "sau",
    "truoc",
    # quantities
    "kg",
    "g",
    "gam",
    "lit",
    "l",
    "ml",
    "ha",
    "m2",
    "m3",
    "%",
    # environment
    "ph",
    "kiem",
    "do",
    "nh3",
    "no2",
    "nhiet",
    "doam",
    "ec",
    # symptoms (detail-ish)
    "vang",
    "heo",
    "kho",
    "khe",
    "tieu",
    "chay",
    "noi",
    "dau",
    "loet",
    "dom",
    "he",
    "ru",
    "rot",
    "mui",
}


def _looks_like_question(text_norm: str) -> bool:
    if not text_norm:
        return False
    # if it contains question mark or typical VN question words
    if "?" in text_norm:
        return True
    return any(w in text_norm.split() for w in ["khong", "sao", "the", "gi", "nao", "bao", "nhu", "vi", "tai"])


def _has_agri_hint(tokens: List[str]) -> bool:
    return any(t in _AGRI_HINT_TOKENS for t in tokens)


def _has_detail(text_norm: str, tokens: List[str]) -> bool:
    if any(ch.isdigit() for ch in text_norm):
        return True
    if any(t in _DETAIL_HINT_TOKENS for t in tokens):
        return True
    # contains units/measure-like strings
    if re.search(r"\b(\d+(?:[\.,]\d+)?)\s*(kg|g|gam|lit|l|ml|ha|m2|m3|ppm|%|Â°c|c)\b", text_norm):
        return True
    return False


def needs_clarification_rule(text: str) -> bool:
    """Heuristic detector for unclear questions.

    Returns True if message is likely agriculture-related but underspecified.
    """

    if not isinstance(text, str):
        return False

    msg = text.strip()
    if not msg:
        return False

    # Skip very long prompts (already detailed)
    if len(msg) > 450:
        return False

    norm = _normalize(msg)
    tokens = _tokenize(norm)

    # If it doesn't look like a question/request, don't intercept.
    if not _looks_like_question(norm):
        return False

    # If not agriculture-related, don't intercept.
    if not _has_agri_hint(tokens):
        return False

    # Extremely short requests are almost always unclear.
    if len(tokens) <= 3:
        return True

    # If generic request pattern AND no detail, treat as unclear.
    if any(re.search(pat, norm) for pat in _GENERIC_PATTERNS) and not _has_detail(norm, tokens):
        return True

    # If message is short-ish and lacks details, treat as unclear.
    if len(tokens) <= 7 and not _has_detail(norm, tokens):
        return True

    return False


@lru_cache(maxsize=1)
def _load_dataset(path: str = DEFAULT_DATASET_PATH) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            out: List[Dict[str, Any]] = []
            for row in data:
                if isinstance(row, dict) and "text" in row and "label" in row:
                    out.append(row)
            return out
        return []
    except Exception:
        return []


@lru_cache(maxsize=1)
def _load_clarify_replies(path: str = DEFAULT_REPLIES_PATH) -> List[str]:
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if not isinstance(data, list):
            return []
        out: List[str] = []
        for row in data:
            if isinstance(row, str):
                t = row.strip()
            elif isinstance(row, dict):
                t = str(row.get("text") or "").strip()
            else:
                continue
            if t:
                out.append(t)
        return out
    except Exception:
        return []


def _resolve_model_path() -> Optional[str]:
    source = (os.environ.get("CLARIFY_INTENT_MODEL_SOURCE") or "auto").strip().lower()
    if source == "off":
        return None
    if source == "local":
        return DEFAULT_MODEL_PATH if os.path.exists(DEFAULT_MODEL_PATH) else None
    # auto
    if os.path.exists(DEFAULT_MODEL_PATH) and os.path.getsize(DEFAULT_MODEL_PATH) > 0:
        return DEFAULT_MODEL_PATH
    return None


def _predict_proba_unclear_ml(text: str) -> Optional[float]:
    """Return P(unclear) if model available.

    Dataset convention: label 0 = unclear, label 1 = clear.
    """
    try:
        model_path = _resolve_model_path()
        if not model_path:
            return None

        with open(model_path, "rb") as f:
            model = pickle.load(f)

        clf = model.get("classifier")
        vec_cfg = model.get("vectorizer") or {}
        if clf is None:
            return None

        from sklearn.feature_extraction.text import HashingVectorizer

        vectorizer = HashingVectorizer(
            n_features=int(vec_cfg.get("n_features", 2**16)),
            alternate_sign=False,
            ngram_range=tuple(vec_cfg.get("ngram_range", (1, 2))),
            norm=vec_cfg.get("norm", "l2"),
        )

        X = vectorizer.transform([text])
        if hasattr(clf, "predict_proba"):
            proba = clf.predict_proba(X)[0]
            classes = list(getattr(clf, "classes_", []))
            if 0 in classes:
                idx = classes.index(0)
                return float(proba[idx])
            # fallback if something is odd
            return float(proba[0])

        return None
    except Exception:
        return None


def needs_clarification(text: str) -> bool:
    """Rule-first, ML-fallback ambiguous question detector."""

    if needs_clarification_rule(text):
        return True

    proba = _predict_proba_unclear_ml(text)
    if proba is None:
        return False

    try:
        thr = float(os.environ.get("CLARIFY_INTENT_THRESHOLD", "0.65"))
    except Exception:
        thr = 0.65

    return proba >= thr


def _detect_domain(text: str) -> str:
    norm = _normalize(text)
    toks = set(_tokenize(norm))

    aqua = {"tom", "ca", "ao", "be", "nuoc", "kiem", "ph", "no2", "nh3"}
    live = {"heo", "lon", "ga", "vit", "bo", "de", "chuong"}

    # NOTE: after _normalize, "phÃ¢n" and "pháº§n" both become "phan".
    # So we must special-case phrases like "kháº©u pháº§n" to avoid misclassifying as crop.
    feed_phrases = ["khau phan", "cong thuc", "phoi tron", "thuc an"]

    crop = {
        "trong",
        "gieo",
        "cay",
        "la",
        "re",
        "than",
        "vuon",
        "ruong",
        "dat",
        # common crops / keywords
        "rau",
        "lua",
        "sau",
        "rieng",
        "caphe",
        "ca",
        "phe",
        "hotieu",
        "ho",
        "tieu",
        "cam",
        "chanh",
        "mit",
        "dua",
        "ot",
        "cachua",
        "ca",
        "chua",
    }
    feed = {"khau", "phan", "cong", "thuc", "phoitr", "tron", "thucan"}

    if toks.intersection(aqua):
        return "aqua"
    if toks.intersection(live):
        return "livestock"
    # If user clearly talks about feed formula/portion, domain is unknown unless explicit animal/aqua tokens exist.
    if any(phrase in norm for phrase in feed_phrases):
        if toks.intersection(aqua):
            return "aqua"
        if toks.intersection(live):
            return "livestock"
        return "unknown"

    if toks.intersection(crop):
        return "crop"

    # If we cannot tell the domain (e.g., "kháº©u pháº§n Äƒn"), treat as unknown.
    if toks.intersection(feed):
        return "unknown"

    return "unknown"


def _detect_topic(domain: str, text: str) -> str:
    """Coarse topic detection to ask the right follow-up questions.

    Returns one of: disease, nutrition, water, odor, technique, unknown
    """

    norm = _normalize(text)
    toks = set(_tokenize(norm))

    # shared
    if any(k in toks for k in {"mui", "hoi"}):
        return "odor"

    if any(k in toks for k in {"khau", "phan", "cong", "thuc", "phoitr", "tron", "thucan", "thuc", "an"}):
        # a bit broad; refined by domain below
        return "nutrition"

    if domain == "unknown":
        return "unknown"

    if domain == "aqua":
        if any(k in toks for k in {"ph", "kiem", "do", "nh3", "no2", "tao", "mau", "nuoc", "bot", "duc"}):
            return "water"
        if any(k in toks for k in {"phan", "trang", "ruot", "dut", "khuc", "mem", "vo", "dom", "loet", "nam"}):
            return "disease"
        return "technique"

    if domain == "livestock":
        if any(k in toks for k in {"ho", "kho", "khe", "tho", "sot", "tieu", "chay", "phan", "bo", "an", "chet"}):
            return "disease"
        return "technique"

    # crop
    if any(k in toks for k in {"phan", "bon", "npk", "vi", "luong", "canxi", "bo", "kali", "dam", "lan"}):
        return "nutrition"
    # IMPORTANT: after normalization, "sáº§u" and "sÃ¢u" both become "sau".
    # So we avoid using bare token "sau" as a disease signal.
    symptom_tokens = {"rep", "nhay", "tri", "ray", "nam", "benh", "dom", "he", "vang", "ru", "rot", "thoi", "xoan", "chay", "ximumu"}
    pest_phrases = ["bi sau", "con sau", "sau an", "sau cuon la", "sau ve bo"]
    if any(k in toks for k in symptom_tokens) or any(p in norm for p in pest_phrases) or any(k in norm for k in ["phun thuoc", "thuoc tri", "tri benh", "phong benh"]):
        return "disease"
    return "technique"


def _is_prefix_safe(domain: str, reply: str) -> bool:
    """Hard filter to prevent obviously off-topic prefixes."""

    r = _normalize(reply)

    crop_terms = ["la ", " re ", " than ", "vuon", "ruong", "cay", "bon phan", "npk", "phun thuoc", "thuoc tri"]
    aqua_terms = ["ao", "be", "do", "kiem", "nh3", "no2", "tao", "mau nuoc", "p h", "ph "]
    live_terms = ["chuong", "dan", "tiem", "tiem phong", "ho", "kho khe", "phan", "bo an"]

    if domain == "livestock":
        # Do not show crop-only wording
        if any(t in r for t in crop_terms):
            return False
        # Avoid water-chem metrics for livestock
        if any(t in r for t in aqua_terms):
            return False

    if domain == "aqua":
        # Avoid crop-only wording
        if any(t in r for t in crop_terms):
            return False

    if domain == "crop":
        # Avoid fish-pond / livestock-specific wording
        if any(t in r for t in aqua_terms):
            return False
        if any(t in r for t in live_terms):
            return False

    return True


def _score_prefix_reply(domain: str, reply: str) -> int:
    """Score a prefix reply to avoid off-topic prompts."""

    r = _normalize(reply)
    score = 0

    # Penalize domain-mismatching nouns.
    if domain != "aqua" and any(x in r for x in ["ao", "be", "tao", "do", "kiem", "nh3", "no2", "mau nuoc"]):
        score -= 4
    if domain != "livestock" and any(x in r for x in ["chuong", "dan", "tiem", "tiem phong", "tieu chay", "ho", "kho khe"]):
        score -= 3
    if domain != "crop" and any(x in r for x in ["la", "re", "than", "vuon", "ruong", "cay"]):
        score -= 3

    # Prefer generic, safe prompts.
    if any(x in r for x in ["doi tuong", "trieu chung", "thoi gian", "khu vuc", "anh"]):
        score += 2
    if "pH" in reply or "EC" in reply or "DO" in reply:
        score += 1 if domain == "aqua" else -2

    return score


_BASE_FALLBACK_REPLIES = [
    "MÃ¬nh cÃ³ thá»ƒ há»— trá»£, nhÆ°ng báº¡n cho mÃ¬nh thÃªm vÃ i chi tiáº¿t Ä‘á»ƒ tÆ° váº¥n Ä‘Ãºng nhÃ©.",
    "Báº¡n mÃ´ táº£ giÃºp mÃ¬nh rÃµ hÆ¡n (Ä‘ang trá»“ng/nuÃ´i gÃ¬, tÃ¬nh tráº¡ng nhÆ° tháº¿ nÃ o, xuáº¥t hiá»‡n bao lÃ¢u rá»“i) nhÃ©.",
]


_PREFIX_BY_DOMAIN: Dict[str, List[str]] = {
    "unknown": [
        "MÃ¬nh cÃ³ thá»ƒ há»— trá»£, nhÆ°ng báº¡n cho mÃ¬nh thÃªm vÃ i chi tiáº¿t Ä‘á»ƒ tÆ° váº¥n Ä‘Ãºng nhÃ©.",
        "Báº¡n nÃ³i rÃµ giÃºp mÃ¬nh Ä‘á»‘i tÆ°á»£ng vÃ  tÃ¬nh tráº¡ng hiá»‡n táº¡i nhÃ©.",
        "Cho mÃ¬nh xin thÃªm 1â€“2 thÃ´ng tin Ä‘á»ƒ mÃ¬nh tÆ° váº¥n sÃ¡t hÆ¡n nha.",
    ],
    "crop": [
        "MÃ¬nh há»— trá»£ Ä‘Æ°á»£c. Báº¡n cho mÃ¬nh xin thÃªm vÃ i chi tiáº¿t Ä‘á»ƒ tÆ° váº¥n Ä‘Ãºng cÃ¢y vÃ  Ä‘Ãºng giai Ä‘oáº¡n nhÃ©.",
        "Báº¡n mÃ´ táº£ ká»¹ hÆ¡n má»™t chÃºt Ä‘á»ƒ mÃ¬nh khÃ´ng tÆ° váº¥n sai hÆ°á»›ng nha.",
        "Báº¡n cho mÃ¬nh thÃªm thÃ´ng tin (hoáº·c áº£nh) Ä‘á»ƒ mÃ¬nh cháº©n Ä‘oÃ¡n sÃ¡t hÆ¡n nhÃ©.",
    ],
    "livestock": [
        "MÃ¬nh há»— trá»£ Ä‘Æ°á»£c. Báº¡n cho mÃ¬nh thÃªm vÃ i chi tiáº¿t vá» Ä‘Ã n vÃ  triá»‡u chá»©ng Ä‘á»ƒ mÃ¬nh tÆ° váº¥n Ä‘Ãºng nhÃ©.",
        "Báº¡n mÃ´ táº£ rÃµ hÆ¡n má»™t chÃºt (Ä‘á»™ tuá»•i, triá»‡u chá»©ng, tá»‰ lá»‡ con bá»‹) Ä‘á»ƒ mÃ¬nh cháº©n Ä‘oÃ¡n sÃ¡t hÆ¡n nha.",
        "Äá»ƒ trÃ¡nh tÆ° váº¥n sai, báº¡n bá»• sung giÃºp mÃ¬nh vÃ i thÃ´ng tin quan trá»ng nhÃ©.",
    ],
    "aqua": [
        "MÃ¬nh há»— trá»£ Ä‘Æ°á»£c. Báº¡n cho mÃ¬nh thÃªm vÃ i thÃ´ng tin vá» ao/bá»ƒ vÃ  hiá»‡n tÆ°á»£ng Ä‘á»ƒ mÃ¬nh tÆ° váº¥n Ä‘Ãºng nhÃ©.",
        "Báº¡n mÃ´ táº£ rÃµ hÆ¡n má»™t chÃºt (loÃ i nuÃ´i, tuá»•i ngÃ y nuÃ´i, biá»ƒu hiá»‡n) Ä‘á»ƒ mÃ¬nh tÆ° váº¥n sÃ¡t hÆ¡n nha.",
        "Cho mÃ¬nh xin thÃªm dá»¯ kiá»‡n vá» nÆ°á»›c vÃ  tÃ¬nh tráº¡ng tÃ´m/cÃ¡ Ä‘á»ƒ mÃ¬nh cháº©n Ä‘oÃ¡n nhanh nhÃ©.",
    ],
}


def generate_clarify_reply(user_text: Optional[str] = None) -> str:
    """Generate a context-aware clarification prompt.

    Key goals:
    - Avoid off-topic questions (no asking pH/EC for pigs, etc.)
    - Ask only the most relevant missing details for the detected domain/topic
    - Keep phrasing friendly and short
    """

    text = user_text or ""
    domain = _detect_domain(text)
    topic = _detect_topic(domain, text)

    # Use curated domain-specific prefixes to avoid off-topic questions.
    prefix_pool = _PREFIX_BY_DOMAIN.get(domain) or _PREFIX_BY_DOMAIN["unknown"]
    prefix = random.choice(prefix_pool)

    # Build a targeted checklist by domain/topic.
    if domain == "unknown":
        if topic == "nutrition":
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: báº¡n cáº§n kháº©u pháº§n/cÃ´ng thá»©c cho Ä‘á»‘i tÆ°á»£ng nÃ o (heo/gÃ /bÃ² hay tÃ´m/cÃ¡), "
                "giai Ä‘oáº¡n (con giá»‘ng/tÄƒng trá»ng/Ä‘áº»; hoáº·c ngÃ y nuÃ´i), vÃ  má»¥c tiÃªu (tÄƒng trá»ng/Ä‘áº»/giá»¯ sá»©c)."
            )
        else:
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: báº¡n Ä‘ang há»i vá» cÃ¢y trá»“ng hay váº­t nuÃ´i/ao nuÃ´i? NÃªu rÃµ Ä‘á»‘i tÆ°á»£ng + tÃ¬nh tráº¡ng + thá»i gian xuáº¥t hiá»‡n Ä‘á»ƒ mÃ¬nh tÆ° váº¥n Ä‘Ãºng."
            )

    elif domain == "aqua":
        if topic == "water":
            norm = _normalize(text)
            is_green_water = any(k in norm for k in ["nuoc ao xanh", "xanh reu", "tao xanh", "tao day", "nuoc xanh", "rong tao"])

            # Water-first questions should ask water context/parameters FIRST.
            # Only ask about tÃ´m/cÃ¡ if the user is actually stocking (optional).
            if is_green_water:
                ask = (
                    "\n\nBáº¡n cho mÃ¬nh xin thÃªm vá» NÆ¯á»šC AO: mÃ u xanh kiá»ƒu gÃ¬ (xanh rÃªu/xanh Ä‘áº­m), Ä‘á»™ trong (Æ°á»›c cm), cÃ³ bá»t/mÃ¹i khÃ´ng, "
                    "vÃ  náº¿u cÃ³ thÃ¬ pH sÃ¡ng/chiá»u, kiá»m, DO (Ä‘áº·c biá»‡t lÃºc 4â€“6h sÃ¡ng), NH3/NO2, nhiá»‡t Ä‘á»™. "
                    "Ao cÃ³ quáº¡t/ sá»¥c khÃ­ khÃ´ng vÃ  gáº§n Ä‘Ã¢y cÃ³ mÆ°a/táº¡t vÃ´i/diá»‡t táº£o/thay nÆ°á»›c khÃ´ng? (Náº¿u ao Ä‘ang nuÃ´i gÃ¬ thÃ¬ nÃ³i thÃªm giÃºp mÃ¬nh.)"
                )
            else:
                ask = (
                    "\n\nBáº¡n cho mÃ¬nh xin thÃªm vá» NÆ¯á»šC AO: diá»‡n tÃ­ch/Ä‘á»™ sÃ¢u ao, mÃ u nÆ°á»›c (xanh/Ä‘á»¥c/nÃ¢u), cÃ³ mÃ¹i/bá»t khÃ´ng, "
                    "vÃ  náº¿u cÃ³ thÃ¬ pH sÃ¡ng/chiá»u, kiá»m, DO, NH3/NO2, nhiá»‡t Ä‘á»™/Ä‘á»™ máº·n. "
                    "Gáº§n Ä‘Ã¢y báº¡n cÃ³ thay nÆ°á»›c/táº¡t vÃ´i/vi sinh/hoÃ¡ cháº¥t gÃ¬ khÃ´ng? (Náº¿u ao Ä‘ang nuÃ´i gÃ¬ thÃ¬ nÃ³i thÃªm giÃºp mÃ¬nh.)"
                )
        elif topic == "nutrition":
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: loÃ i nuÃ´i, tuá»•i ngÃ y nuÃ´i, lÆ°á»£ng cho Äƒn/ngÃ y (máº¥y cá»¯), biá»ƒu hiá»‡n Ä‘Æ°á»ng ruá»™t/phÃ¢n (náº¿u cÃ³), "
                "vÃ  gáº§n Ä‘Ã¢y cÃ³ Ä‘á»•i cÃ¡m/men/vi sinh gÃ¬ khÃ´ng."
            )
        else:
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: loÃ i nuÃ´i (tÃ´m/cÃ¡), tuá»•i ngÃ y nuÃ´i, triá»‡u chá»©ng chÃ­nh, vÃ  cÃ¡c thÃ´ng sá»‘ nÆ°á»›c cÆ¡ báº£n (pHâ€“kiá»mâ€“DOâ€“nhiá»‡t) náº¿u cÃ³."
            )

    elif domain == "livestock":
        if topic == "odor":
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: loáº¡i chuá»“ng (kÃ­n/há»Ÿ), ná»n khÃ´ hay Æ°á»›t, cÃ³ rÃ² nÆ°á»›c uá»‘ng khÃ´ng, sá»‘ lÆ°á»£ng con, "
                "vÃ  báº¡n Ä‘ang dÃ¹ng Ä‘á»‡m lÃ³t/men vi sinh/khá»­ mÃ¹i gÃ¬ rá»“i."
            )
        elif topic == "nutrition":
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: con gÃ¬ (heo/gÃ /bÃ²...), lá»©a tuá»•i/giai Ä‘oáº¡n (con giá»‘ng/thá»‹t/Ä‘áº»), má»¥c tiÃªu (tÄƒng trá»ng/Ä‘áº»), "
                "kháº©u pháº§n hiá»‡n táº¡i (loáº¡i cÃ¡m/tá»‰ lá»‡ phá»‘i trá»™n) vÃ  biá»ƒu hiá»‡n (gáº§y/yáº¿u/tiÃªu cháº£y...) náº¿u cÃ³."
            )
        else:  # disease/technique
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: con gÃ¬, Ä‘á»™ tuá»•i/trá»ng lÆ°á»£ng, triá»‡u chá»©ng chÃ­nh (ho/khÃ² khÃ¨/tiÃªu cháº£y/bá» Äƒn/sá»‘t), "
                "tá»‰ lá»‡ con bá»‹, vÃ  Ä‘Ã£ tiÃªm phÃ²ng/dÃ¹ng thuá»‘c gÃ¬ gáº§n Ä‘Ã¢y chÆ°a."
            )

    else:  # crop
        if topic == "nutrition":
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: cÃ¢y gÃ¬/giá»‘ng gÃ¬, giai Ä‘oáº¡n (cÃ¢y con/ra hoa/Ä‘áº­u trÃ¡i), triá»‡u chá»©ng (vÃ ng lÃ¡ gÃ¢n xanh/chÃ¡y mÃ©p/rá»¥ng bÃ´ng...), "
                "vÃ  lá»‹ch bÃ³n gáº§n Ä‘Ã¢y (tÃªn phÃ¢n + liá»u). Náº¿u cÃ³ pH Ä‘áº¥t/EC cÃ ng tá»‘t."
            )
        elif topic == "disease":
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: cÃ¢y gÃ¬, giai Ä‘oáº¡n, dáº¥u hiá»‡u cá»¥ thá»ƒ (Ä‘á»‘m dáº¡ng gÃ¬, cÃ³ sÃ¢u/rá»‡p nhÃ¬n tháº¥y khÃ´ng, máº·t trÃªn/dÆ°á»›i lÃ¡), "
                "thá»i gian xuáº¥t hiá»‡n vÃ  báº¡n Ä‘Ã£ phun/bÃ³n gÃ¬ gáº§n Ä‘Ã¢y. Náº¿u cÃ³ áº£nh cáº­n cáº£nh + tá»•ng quan cÃ ng tá»‘t."
            )
        else:  # technique
            ask = (
                "\n\nBáº¡n cho mÃ¬nh xin thÃªm: báº¡n Ä‘á»‹nh trá»“ng cÃ¢y gÃ¬, trá»“ng á»Ÿ Ä‘Ã¢u (cháº­u/luá»‘ng/vÆ°á»n), Ä‘iá»u kiá»‡n (náº¯ng/mÆ°a, Ä‘áº¥t/cÃ¡t/phÃ¨n), "
                "vÃ  má»¥c tiÃªu (trá»“ng Äƒn lÃ¡/láº¥y trÃ¡i/quy mÃ´) Ä‘á»ƒ mÃ¬nh hÆ°á»›ng dáº«n Ä‘Ãºng." 
            )

    return prefix + ask


def cli_train(args: argparse.Namespace) -> int:
    data = _load_dataset(args.dataset)

    texts: List[str] = []
    labels: List[int] = []

    for row in data:
        t = str(row.get("text") or "").strip()
        y = row.get("label")
        if not t:
            continue
        if y not in (0, 1, True, False):
            continue
        texts.append(t)
        labels.append(int(bool(y)))

    if not texts:
        raise ValueError("Empty clarify dataset")

    from sklearn.feature_extraction.text import HashingVectorizer
    from sklearn.linear_model import SGDClassifier

    vectorizer = HashingVectorizer(
        n_features=2**16,
        alternate_sign=False,
        ngram_range=(1, 2),
        norm="l2",
    )

    X = vectorizer.transform(texts)

    clf = SGDClassifier(
        loss="log_loss",
        alpha=1e-4,
        max_iter=30,
        tol=1e-3,
        random_state=int(args.seed),
    )
    clf.fit(X, labels)

    pred = clf.predict(X)
    acc = sum(1 for p, y in zip(pred, labels) if int(p) == int(y)) / max(1, len(labels))

    os.makedirs(os.path.dirname(os.path.abspath(args.model_out)), exist_ok=True)
    with open(args.model_out, "wb") as f:
        pickle.dump(
            {
                "type": "hashing_sgd_clarify_intent_v1",
                "vectorizer": {"n_features": 2**16, "ngram_range": (1, 2), "norm": "l2"},
                "classifier": clf,
                "label_meaning": {"0": "unclear", "1": "clear"},
            },
            f,
        )

    print(f"âœ… samples: {len(texts)}")
    print(f"âœ… train_acc (sanity): {acc:.3f}")
    print(f"ðŸ’¾ saved: {os.path.abspath(args.model_out)}")
    return 0


def build_argparser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(description="Clarify intent detector + clarification replies")
    p.add_argument("--dataset", default=DEFAULT_DATASET_PATH)
    p.add_argument("--seed", type=int, default=42)

    sub = p.add_subparsers(dest="cmd")

    p_train = sub.add_parser("train", help="Train clarify intent model")
    p_train.add_argument("--model-out", default=DEFAULT_MODEL_PATH)
    p_train.set_defaults(func=cli_train)

    return p


def main() -> int:
    parser = build_argparser()
    args = parser.parse_args()

    if hasattr(args, "func"):
        return int(args.func(args))

    parser.print_help()
    return 2


if __name__ == "__main__":
    raise SystemExit(main())
