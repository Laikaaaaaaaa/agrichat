#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AgriSense AI - Agricultural Question Analysis & Prompt Builder
==============================================================
File: main.py
Author: AgriSense AI Team
Description: Pipeline phÃ¢n tÃ­ch cÃ¢u há»i nÃ´ng nghiá»‡p vÃ  táº¡o prompt cho OpenAI API

Cháº¡y:
  - Interactive mode: python main.py
  - Train mode: python main.py --mode train
"""

import argparse
import json
import math
import os
import pickle
import random
import re
from datetime import datetime
from typing import Optional, Dict, List, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

RANDOM_SEED = 42
random.seed(RANDOM_SEED)

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.linear_model import SGDClassifier
    from sklearn.pipeline import Pipeline
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
MODELS_DIR = os.path.join(BASE_DIR, "models")
LOGS_DIR = os.path.join(BASE_DIR, "logs")
TRAIN_FILE = os.path.join(DATA_DIR, "train_cases.jsonl")
MODEL_FILE = os.path.join(MODELS_DIR, "crop_classifier.pkl")
CONVERSATIONS_LOG = os.path.join(LOGS_DIR, "conversations.jsonl")
TRAIN_METRICS_LOG = os.path.join(LOGS_DIR, "train_metrics.json")


def ensure_directories():
    for d in [DATA_DIR, MODELS_DIR, LOGS_DIR]:
        os.makedirs(d, exist_ok=True)


class Region(Enum):
    DBSCL = "Äá»“ng báº±ng sÃ´ng Cá»­u Long"
    TAY_NGUYEN = "TÃ¢y NguyÃªn"
    MIEN_BAC = "Miá»n Báº¯c"
    MIEN_TRUNG = "Miá»n Trung"
    DONG_NAM_BO = "ÄÃ´ng Nam Bá»™"
    UNKNOWN = "KhÃ´ng xÃ¡c Ä‘á»‹nh"


class Season(Enum):
    MUA = "MÃ¹a mÆ°a"
    KHO = "MÃ¹a khÃ´"
    DONG_XUAN = "ÄÃ´ng XuÃ¢n"
    HE_THU = "HÃ¨ Thu"
    THU_DONG = "Thu ÄÃ´ng"
    UNKNOWN = "KhÃ´ng rÃµ"


class Scale(Enum):
    NHA_VUON = "NhÃ  vÆ°á»n/Há»™ gia Ä‘Ã¬nh"
    TRANG_TRAI = "Trang tráº¡i"
    UNKNOWN = "KhÃ´ng xÃ¡c Ä‘á»‹nh"


class Experience(Enum):
    PHO_THONG = "NÃ´ng dÃ¢n phá»• thÃ´ng"
    CO_KINH_NGHIEM = "NgÆ°á»i cÃ³ kinh nghiá»‡m"
    UNKNOWN = "KhÃ´ng xÃ¡c Ä‘á»‹nh"


@dataclass
class QuestionAnalysis:
    original_question: str
    crop: Optional[str] = None
    stage: Optional[str] = None
    symptoms: List[str] = None
    region: str = Region.UNKNOWN.value
    season: str = Season.UNKNOWN.value
    scale: str = Scale.UNKNOWN.value
    experience: str = Experience.UNKNOWN.value
    weather_context: Optional[str] = None
    time_context: Optional[str] = None
    action_asked: Optional[str] = None
    urgency_level: str = "normal"

    def __post_init__(self):
        if self.symptoms is None:
            self.symptoms = []

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class AgriLogicResult:
    priority_causes: List[str]
    secondary_causes: List[str]
    recommended_actions: List[str]
    avoid_actions: List[str]
    check_first: List[str]
    knowledge_notes: List[str]
    confidence_level: str
    reasoning_chain: List[str]

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


CROP_KEYWORDS = {
    "lÃºa": ["lÃºa", "lua", "náº¿p", "gáº¡o", "ruá»™ng lÃºa", "Ä‘á»“ng lÃºa"],
    "cÃ  phÃª": ["cÃ  phÃª", "ca phe", "cafe", "cÃ  fÃª", "coffee"],
    "tiÃªu": ["tiÃªu", "há»“ tiÃªu", "ho tieu", "pepper"],
    "Ä‘iá»u": ["Ä‘iá»u", "Ä‘Ã o lá»™n há»™t", "cashew"],
    "cao su": ["cao su", "cÃ¢y su"],
    "sáº§u riÃªng": ["sáº§u riÃªng", "sau rieng", "durian", "sáº§u", "monthong", "ri6"],
    "bÆ°á»Ÿi": ["bÆ°á»Ÿi", "buoi"],
    "cam": ["cam", "quÃ½t", "chanh"],
    "xoÃ i": ["xoÃ i", "xoai", "mango"],
    "nhÃ£n": ["nhÃ£n", "nhan", "longan"],
    "váº£i": ["váº£i", "vai", "lychee"],
    "thanh long": ["thanh long", "dragon fruit"],
    "chuá»‘i": ["chuá»‘i", "chuoi", "banana"],
    "dÆ°a háº¥u": ["dÆ°a háº¥u", "dua hau", "watermelon"],
    "dÆ°a leo": ["dÆ°a leo", "dua leo", "cucumber", "dÆ°a chuá»™t"],
    "rau muá»‘ng": ["rau muá»‘ng", "rau muong"],
    "rau cáº£i": ["rau cáº£i", "cáº£i", "cáº£i báº¯p", "cáº£i tháº£o", "cáº£i xanh", "báº¯p cáº£i", "cabbage"],
    "cÃ  chua": ["cÃ  chua", "ca chua", "tomato"],
    "á»›t": ["á»›t", "ot", "chili"],
    "khoai": ["khoai", "khoai lang", "khoai tÃ¢y", "khoai mÃ¬", "sáº¯n"],
    "ngÃ´": ["ngÃ´", "báº¯p", "ngo", "corn", "maize"],
    "Ä‘áº­u": ["Ä‘áº­u", "Ä‘áº­u nÃ nh", "Ä‘áº­u phá»™ng", "Ä‘áº­u xanh", "Ä‘áº­u Ä‘en", "bean"],
    "mÃ­a": ["mÃ­a", "mia", "sugarcane"],
    "dá»«a": ["dá»«a", "dua", "coconut"],
    "hoa": ["hoa", "hoa cÃºc", "hoa há»“ng", "hoa lan", "hoa mai"],
}

STAGE_KEYWORDS = {
    "gieo máº¡": ["gieo máº¡", "gieo ma", "máº¡", "Æ°Æ¡m giá»‘ng", "ngÃ¢m giá»‘ng"],
    "Ä‘áº» nhÃ¡nh": ["Ä‘áº» nhÃ¡nh", "de nhanh", "náº£y chá»“i", "Ä‘Ã¢m chá»“i"],
    "lÃ m Ä‘Ã²ng": ["lÃ m Ä‘Ã²ng", "lam dong", "trá»• Ä‘Ã²ng", "Ä‘á»©ng cÃ¡i"],
    "trá»• bÃ´ng": ["trá»• bÃ´ng", "tro bong", "trá»•", "phÆ¡i mÃ u"],
    "ngáº­m sá»¯a": ["ngáº­m sá»¯a", "ngam sua", "vÃ o cháº¯c"],
    "chÃ­n": ["chÃ­n", "chin", "thu hoáº¡ch", "gáº·t"],
    "ra hoa": ["ra hoa", "ra bÃ´ng", "ná»Ÿ hoa", "Ä‘áº­u hoa"],
    "Ä‘áº­u trÃ¡i": ["Ä‘áº­u trÃ¡i", "Ä‘áº­u quáº£", "káº¿t trÃ¡i"],
    "nuÃ´i trÃ¡i": ["nuÃ´i trÃ¡i", "nuÃ´i quáº£", "phÃ¡t triá»ƒn quáº£"],
    "trÃ¡i non": ["trÃ¡i non", "quáº£ non"],
    "trÃ¡i giÃ ": ["trÃ¡i giÃ ", "quáº£ giÃ ", "sáº¯p chÃ­n"],
    "cÃ¢y con": ["cÃ¢y con", "cÃ¢y giá»‘ng", "má»›i trá»“ng", "má»›i xuá»‘ng giá»‘ng"],
    "sinh trÆ°á»Ÿng": ["sinh trÆ°á»Ÿng", "phÃ¡t triá»ƒn", "lá»›n lÃªn"],
    "ra lÃ¡": ["ra lÃ¡", "má»c lÃ¡", "lÃ¡ non"],
    "ra rá»…": ["ra rá»…", "bÃ©n rá»…", "phÃ¡t triá»ƒn rá»…"],
}

SYMPTOM_KEYWORDS = {
    "vÃ ng lÃ¡": ["vÃ ng lÃ¡", "lÃ¡ vÃ ng", "lÃ¡ Ãºa", "lÃ¡ hÃ©o vÃ ng", "vÃ ng háº¿t", "vÃ ng dáº§n"],
    "vÃ ng lÃ¡ tá»« gá»‘c": ["vÃ ng tá»« gá»‘c", "vÃ ng lÃ¡ tá»« gá»‘c", "vÃ ng tá»« dÆ°á»›i lÃªn", "vÃ ng tá»« gá»‘c lÃªn"],
    "vÃ ng lÃ¡ tá»« ngá»n": ["vÃ ng tá»« ngá»n", "vÃ ng lÃ¡ tá»« ngá»n", "vÃ ng tá»« trÃªn xuá»‘ng"],
    "chÃ¡y lÃ¡": ["chÃ¡y lÃ¡", "lÃ¡ chÃ¡y", "khÃ´ lÃ¡", "lÃ¡ khÃ´", "chÃ¡y rÃ¬a", "chÃ¡y mÃ©p lÃ¡"],
    "Ä‘á»‘m lÃ¡": ["Ä‘á»‘m lÃ¡", "lÃ¡ Ä‘á»‘m", "váº¿t Ä‘á»‘m", "Ä‘á»‘m nÃ¢u", "Ä‘á»‘m vÃ ng", "Ä‘á»‘m Ä‘en", "cháº¥m lÃ¡"],
    "thá»‘i rá»…": ["thá»‘i rá»…", "rá»… thá»‘i", "hÆ° rá»…", "rá»… Ä‘en", "rá»… má»m", "rá»… nhÅ©n"],
    "ngáº­p Ãºng": ["Ãºng", "ngáº­p Ãºng", "ngáº­p nÆ°á»›c", "ngáº­p", "Ãºng nÆ°á»›c", "Ä‘á»ng nÆ°á»›c", "nÆ°á»›c ngáº­p"],
    "xoÄƒn lÃ¡": ["xoÄƒn lÃ¡", "lÃ¡ xoÄƒn", "cuá»‘n lÃ¡", "lÃ¡ cuá»‘n", "lÃ¡ quÄƒn", "quÄƒn lÃ¡"],
    "hÃ©o": ["hÃ©o", "rÅ©", "xÃ¬u", "má»m nhÅ©n", "hÃ©o rÅ©", "hÃ©o dáº§n", "hÃ©o xanh"],
    "rá»¥ng lÃ¡": ["rá»¥ng lÃ¡", "rÆ¡i lÃ¡", "lÃ¡ rá»¥ng", "lÃ¡ rÆ¡i"],
    "rá»¥ng hoa": ["rá»¥ng hoa", "rÆ¡i hoa", "hoa rá»¥ng", "hoa rÆ¡i", "khÃ´ng Ä‘áº­u hoa"],
    "rá»¥ng trÃ¡i": ["rá»¥ng trÃ¡i", "rÆ¡i trÃ¡i", "trÃ¡i rá»¥ng", "quáº£ rá»¥ng", "trÃ¡i rÆ¡i"],
    "sÃ¢u": ["sÃ¢u", "sÃ¢u Ä‘á»¥c", "sÃ¢u Äƒn", "sÃ¢u cuá»‘n", "sÃ¢u tÆ¡", "sÃ¢u xanh", "sÃ¢u keo"],
    "ráº§y": ["ráº§y", "ráº§y nÃ¢u", "ráº§y xanh", "ráº§y lÆ°ng tráº¯ng", "ráº§y chá»•ng cÃ¡nh"],
    "bá»": ["bá»", "bá» xÃ­t", "bá» trÄ©", "bá» cÃ¡nh cá»©ng", "bá» nháº£y", "bá» hÃ "],
    "rá»‡p": ["rá»‡p", "rá»‡p sÃ¡p", "rá»‡p váº£y", "rá»‡p muá»™i"],
    "nhá»‡n": ["nhá»‡n", "nhá»‡n Ä‘á»", "nhá»‡n giÃ©"],
    "náº¥m": ["náº¥m", "náº¥m bá»‡nh", "má»‘c", "pháº¥n tráº¯ng", "gá»‰ sáº¯t", "thÃ¡n thÆ°", "Ä‘áº¡o Ã´n"],
    "vi khuáº©n": ["vi khuáº©n", "thá»‘i nhÅ©n", "cháº£y nhá»±a", "báº¡c lÃ¡"],
    "virus": ["kháº£m", "xoÄƒn lÃ¹n", "lÃ¹n sá»c Ä‘en", "vÃ ng lÃ¹n"],
    "cháº­m lá»›n": ["cháº­m lá»›n", "cÃ²i cá»c", "khÃ´ng phÃ¡t triá»ƒn", "lÃ¹n", "khÃ´ng lá»›n", "cháº­m phÃ¡t triá»ƒn"],
    "thiáº¿u dinh dÆ°á»¡ng": ["thiáº¿u dinh dÆ°á»¡ng", "thiáº¿u phÃ¢n", "thiáº¿u Ä‘áº¡m", "thiáº¿u lÃ¢n", "thiáº¿u kali", "thiáº¿u vi lÆ°á»£ng"],
    "khÃ´ hÃ©o": ["khÃ´ hÃ©o", "khÃ´ dáº§n", "khÃ´ cÃ nh", "cháº¿t khÃ´", "khÃ´ Ä‘á»t"],
    "thá»‘i thÃ¢n": ["thá»‘i thÃ¢n", "thÃ¢n thá»‘i", "thá»‘i gá»‘c", "gá»‘c thá»‘i", "má»¥c thÃ¢n"],
    "xÃ¬ má»§": ["xÃ¬ má»§", "cháº£y má»§", "cháº£y nhá»±a", "tiáº¿t má»§"],
    "Ä‘áº¡o Ã´n": ["Ä‘áº¡o Ã´n", "chÃ¡y lÃ¡", "khÃ´ váº±n", "Ä‘á»‘m cá»• bÃ´ng"],
    "lem lÃ©p": ["lem lÃ©p", "lÃ©p háº¡t", "háº¡t lÃ©p", "lÃ©p lá»­ng"],
}

REGION_KEYWORDS = {
    Region.DBSCL: ["Ä‘á»“ng báº±ng sÃ´ng cá»­u long", "dbscl", "miá»n tÃ¢y", "cáº§n thÆ¡", "an giang",
                   "kiÃªn giang", "Ä‘á»“ng thÃ¡p", "long an", "tiá»n giang", "báº¿n tre",
                   "vÄ©nh long", "trÃ  vinh", "sÃ³c trÄƒng", "háº­u giang", "báº¡c liÃªu", "cÃ  mau"],
    Region.TAY_NGUYEN: ["tÃ¢y nguyÃªn", "Ä‘áº¯k láº¯k", "Ä‘áº¯k nÃ´ng", "gia lai", "kon tum", "lÃ¢m Ä‘á»“ng",
                        "ban mÃª thuá»™t", "pleiku", "Ä‘Ã  láº¡t"],
    Region.MIEN_BAC: ["miá»n báº¯c", "hÃ  ná»™i", "háº£i phÃ²ng", "nam Ä‘á»‹nh", "thÃ¡i bÃ¬nh", "hÆ°ng yÃªn",
                      "háº£i dÆ°Æ¡ng", "báº¯c ninh", "vÄ©nh phÃºc", "phÃº thá»", "Ä‘á»“ng báº±ng sÃ´ng há»“ng"],
    Region.MIEN_TRUNG: ["miá»n trung", "Ä‘Ã  náºµng", "huáº¿", "quáº£ng nam", "quáº£ng ngÃ£i", "bÃ¬nh Ä‘á»‹nh",
                        "phÃº yÃªn", "khÃ¡nh hÃ²a", "ninh thuáº­n", "bÃ¬nh thuáº­n", "nghá»‡ an", "hÃ  tÄ©nh"],
    Region.DONG_NAM_BO: ["Ä‘Ã´ng nam bá»™", "bÃ¬nh dÆ°Æ¡ng", "Ä‘á»“ng nai", "bÃ  rá»‹a", "vÅ©ng tÃ u",
                         "bÃ¬nh phÆ°á»›c", "tÃ¢y ninh", "sÃ i gÃ²n", "tp hcm", "há»“ chÃ­ minh"],
}

WEATHER_KEYWORDS = {
    "mÆ°a": ["mÆ°a", "mua", "mÆ°a nhiá»u", "mÆ°a hoÃ i", "mÆ°a dáº§m", "ngáº­p"],
    "náº¯ng": ["náº¯ng", "nang", "náº¯ng gáº¯t", "náº¯ng nÃ³ng", "khÃ´ háº¡n"],
    "láº¡nh": ["láº¡nh", "rÃ©t", "láº¡nh giÃ¡", "sÆ°Æ¡ng muá»‘i"],
    "giÃ³": ["giÃ³", "bÃ£o", "giÃ´ng", "giÃ³ lá»›n"],
    "áº©m": ["áº©m", "áº©m Æ°á»›t", "Ä‘á»™ áº©m cao"],
}

ACTION_KEYWORDS = {
    "bÃ³n phÃ¢n": ["bÃ³n phÃ¢n", "bÃ³n thÃªm phÃ¢n", "phun phÃ¢n", "bá»• sung phÃ¢n"],
    "phun thuá»‘c": ["phun thuá»‘c", "xá»‹t thuá»‘c", "thuá»‘c trá»« sÃ¢u", "thuá»‘c bá»‡nh"],
    "tÆ°á»›i nÆ°á»›c": ["tÆ°á»›i", "tÆ°á»›i nÆ°á»›c", "tÆ°á»›i thÃªm"],
    "cáº¯t tá»‰a": ["cáº¯t tá»‰a", "tá»‰a cÃ nh", "cáº¯t bá»"],
    "thu hoáº¡ch": ["thu hoáº¡ch", "gáº·t", "hÃ¡i"],
}


class CropClassifier:
    """
    Advanced Crop Classifier with proper ML training
    - SGDClassifier with partial_fit for incremental learning
    - Learning rate scheduling
    - Keyword weight optimization
    """
    def __init__(self):
        self.model = None
        self.vectorizer = None
        self.classes = list(CROP_KEYWORDS.keys())
        self.keyword_weights = {crop: 1.0 for crop in self.classes}
        self.keyword_match_counts = {crop: {"correct": 0, "total": 0} for crop in self.classes}
        self.train_history = {"loss": [], "accuracy": []}
        self.learning_rate = 0.01
        self.epoch_count = 0

    def _build_sklearn_model(self):
        if HAS_SKLEARN:
            return SGDClassifier(
                loss='log_loss',
                penalty='l2',
                alpha=0.0001,
                learning_rate='optimal',
                eta0=self.learning_rate,
                max_iter=1,
                warm_start=True,
                random_state=RANDOM_SEED
            )
        return None

    def _ensure_vectorizer(self, texts: List[str]):
        if HAS_SKLEARN and self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                ngram_range=(1, 3),
                max_features=1000,
                sublinear_tf=True,
                min_df=1
            )
            self.vectorizer.fit(texts)

    def partial_fit(self, texts: List[str], labels: List[str], learning_rate: float = None):
        """Incremental training - learns from batch without forgetting"""
        if learning_rate:
            self.learning_rate = learning_rate
        
        self.epoch_count += 1
        
        if HAS_SKLEARN:
            self._ensure_vectorizer(texts)
            X = self.vectorizer.transform(texts)
            
            # Collect all unique labels seen so far
            for label in labels:
                if label not in self.classes:
                    self.classes.append(label)
            
            # Get all possible classes (sorted for consistency)
            all_classes = sorted(list(set(self.classes)))
            
            if self.model is None:
                self.model = self._build_sklearn_model()
                self.model.partial_fit(X, labels, classes=all_classes)
            else:
                # Check if we have new classes
                current_classes = set(self.model.classes_) if hasattr(self.model, 'classes_') else set()
                new_classes = set(labels) - current_classes
                if new_classes:
                    # Need to reinitialize model with new classes
                    all_classes = sorted(list(current_classes | new_classes | set(self.classes)))
                    old_model = self.model
                    self.model = self._build_sklearn_model()
                    self.model.partial_fit(X, labels, classes=all_classes)
                else:
                    self.model.partial_fit(X, labels)
            
            # Calculate loss
            try:
                proba = self.model.predict_proba(X)
                # Cross entropy loss
                loss = 0.0
                for i, label in enumerate(labels):
                    if label in self.model.classes_:
                        label_idx = list(self.model.classes_).index(label)
                        prob = max(proba[i][label_idx], 1e-10)
                        loss -= math.log(prob)
                    else:
                        loss += 2.0  # Penalty for unknown label
                loss /= len(labels)
                self.train_history["loss"].append(loss)
            except Exception:
                self.train_history["loss"].append(0.5)
        
        # Also update keyword weights (hybrid approach)
        for text, label in zip(texts, labels):
            normalized = text.lower()
            for crop, keywords in CROP_KEYWORDS.items():
                matched = any(kw in normalized for kw in keywords)
                if matched:
                    self.keyword_match_counts[crop]["total"] += 1
                    if crop == label:
                        self.keyword_match_counts[crop]["correct"] += 1
                        # Increase weight for correct matches
                        delta = self.learning_rate * 0.5
                        self.keyword_weights[crop] = min(3.0, self.keyword_weights.get(crop, 1.0) + delta)
                    else:
                        # Decrease weight for incorrect matches
                        delta = self.learning_rate * 0.2
                        self.keyword_weights[crop] = max(0.3, self.keyword_weights.get(crop, 1.0) - delta)

    def fit(self, texts: List[str], labels: List[str]):
        """Full training - resets and trains from scratch"""
        self.partial_fit(texts, labels)

    def predict(self, text: str) -> Optional[str]:
        normalized = text.lower()
        
        # Try sklearn model first
        if HAS_SKLEARN and self.model is not None and self.vectorizer is not None:
            try:
                X = self.vectorizer.transform([text])
                pred = self.model.predict(X)[0]
                # Verify with keyword check
                for crop, keywords in CROP_KEYWORDS.items():
                    if any(kw in normalized for kw in keywords):
                        if crop == pred:
                            return pred
                        # If mismatch, use weighted scoring
                        break
                return pred
            except Exception:
                pass
        
        # Fallback: Weighted keyword matching
        best_crop = None
        best_score = 0.0
        for crop, keywords in CROP_KEYWORDS.items():
            for kw in keywords:
                if kw in normalized:
                    weight = self.keyword_weights.get(crop, 1.0)
                    # Bonus for longer keyword matches
                    length_bonus = len(kw) / 10.0
                    score = weight + length_bonus
                    if score > best_score:
                        best_score = score
                        best_crop = crop
                    break
        return best_crop

    def predict_proba(self, text: str) -> Dict[str, float]:
        """Get probability distribution over crops"""
        if HAS_SKLEARN and self.model is not None and self.vectorizer is not None:
            try:
                X = self.vectorizer.transform([text])
                proba = self.model.predict_proba(X)[0]
                return {cls: float(p) for cls, p in zip(self.model.classes_, proba)}
            except Exception:
                pass
        
        # Fallback: normalize keyword weights
        normalized = text.lower()
        scores = {}
        for crop, keywords in CROP_KEYWORDS.items():
            if any(kw in normalized for kw in keywords):
                scores[crop] = self.keyword_weights.get(crop, 1.0)
        
        if scores:
            total = sum(scores.values())
            return {k: v/total for k, v in scores.items()}
        return {}

    def evaluate(self, texts: List[str], labels: List[str]) -> Dict[str, float]:
        """Evaluate model on a dataset"""
        correct = 0
        total = len(texts)
        
        for text, label in zip(texts, labels):
            pred = self.predict(text)
            if pred and pred.lower() == label.lower():
                correct += 1
        
        accuracy = correct / total if total > 0 else 0.0
        self.train_history["accuracy"].append(accuracy)
        return {"accuracy": accuracy, "correct": correct, "total": total}

    def get_loss(self) -> float:
        """Get latest training loss"""
        if self.train_history["loss"]:
            return self.train_history["loss"][-1]
        return 1.0

    def save(self, path: str):
        with open(path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'vectorizer': self.vectorizer,
                'weights': self.keyword_weights,
                'classes': self.classes,
                'match_counts': self.keyword_match_counts,
                'history': self.train_history,
                'epoch_count': self.epoch_count
            }, f)

    def load(self, path: str):
        if os.path.exists(path):
            with open(path, 'rb') as f:
                data = pickle.load(f)
                self.model = data.get('model')
                self.vectorizer = data.get('vectorizer')
                self.keyword_weights = data.get('weights', self.keyword_weights)
                self.classes = data.get('classes', self.classes)
                self.keyword_match_counts = data.get('match_counts', self.keyword_match_counts)
                self.train_history = data.get('history', self.train_history)
                self.epoch_count = data.get('epoch_count', 0)
            return True
        return False


crop_classifier = CropClassifier()


class SymptomClassifier:
    """
    Multi-label classifier for symptoms
    Uses TF-IDF + Binary Relevance approach
    """
    def __init__(self):
        self.vectorizer = None
        self.classifiers = {}  # One classifier per symptom
        self.symptom_classes = list(SYMPTOM_KEYWORDS.keys())
        self.symptom_weights = {s: 1.0 for s in self.symptom_classes}
        self.epoch_count = 0
        self.learning_rate = 0.01
    
    def _ensure_vectorizer(self, texts: List[str]):
        if HAS_SKLEARN and self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(
                ngram_range=(1, 2),
                max_features=500,
                sublinear_tf=True
            )
            self.vectorizer.fit(texts)
    
    def partial_fit(self, texts: List[str], labels_list: List[List[str]], learning_rate: float = None):
        """Train symptom classifiers incrementally"""
        if learning_rate:
            self.learning_rate = learning_rate
        self.epoch_count += 1
        
        if HAS_SKLEARN:
            self._ensure_vectorizer(texts)
            X = self.vectorizer.transform(texts)
            
            # Train one classifier per symptom (Binary Relevance)
            for symptom in self.symptom_classes:
                # Create binary labels
                y = [1 if symptom in labels else 0 for labels in labels_list]
                
                if sum(y) == 0:  # Skip if no positive examples
                    continue
                
                if symptom not in self.classifiers:
                    self.classifiers[symptom] = SGDClassifier(
                        loss='log_loss',
                        penalty='l2',
                        max_iter=1,
                        warm_start=True,
                        random_state=RANDOM_SEED
                    )
                
                self.classifiers[symptom].partial_fit(X, y, classes=[0, 1])
        
        # Update keyword weights
        for text, symptoms in zip(texts, labels_list):
            normalized = text.lower()
            for symptom, keywords in SYMPTOM_KEYWORDS.items():
                matched = any(kw in normalized for kw in keywords)
                if matched:
                    if symptom in symptoms:
                        delta = self.learning_rate * 0.3
                        self.symptom_weights[symptom] = min(2.5, self.symptom_weights.get(symptom, 1.0) + delta)
                    else:
                        delta = self.learning_rate * 0.1
                        self.symptom_weights[symptom] = max(0.5, self.symptom_weights.get(symptom, 1.0) - delta)
    
    def predict(self, text: str) -> List[str]:
        """Predict symptoms for a text"""
        predictions = []
        
        if HAS_SKLEARN and self.vectorizer is not None:
            try:
                X = self.vectorizer.transform([text])
                for symptom, clf in self.classifiers.items():
                    proba = clf.predict_proba(X)[0]
                    if len(proba) > 1 and proba[1] > 0.3:  # Threshold
                        predictions.append(symptom)
            except Exception:
                pass
        
        # Also use keyword matching with weights
        normalized = text.lower()
        for symptom, keywords in SYMPTOM_KEYWORDS.items():
            for keyword in keywords:
                if keyword in normalized:
                    weight = self.symptom_weights.get(symptom, 1.0)
                    if weight > 0.8 and symptom not in predictions:
                        predictions.append(symptom)
                    break
        
        return predictions
    
    def evaluate(self, texts: List[str], labels_list: List[List[str]]) -> Dict[str, float]:
        """Evaluate symptom prediction"""
        total_precision = 0
        total_recall = 0
        count = 0
        
        for text, gold in zip(texts, labels_list):
            pred = self.predict(text)
            
            if gold:
                # Calculate metrics
                matched = sum(1 for g in gold if any(g in p or p in g for p in pred))
                precision = matched / len(pred) if pred else 0
                recall = matched / len(gold)
                total_precision += precision
                total_recall += recall
                count += 1
        
        avg_precision = total_precision / count if count > 0 else 0
        avg_recall = total_recall / count if count > 0 else 0
        f1 = 2 * avg_precision * avg_recall / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0
        
        return {"precision": avg_precision, "recall": avg_recall, "f1": f1}
    
    def save(self, path: str):
        with open(path, 'wb') as f:
            pickle.dump({
                'vectorizer': self.vectorizer,
                'classifiers': self.classifiers,
                'weights': self.symptom_weights,
                'epoch_count': self.epoch_count
            }, f)
    
    def load(self, path: str):
        if os.path.exists(path):
            with open(path, 'rb') as f:
                data = pickle.load(f)
                self.vectorizer = data.get('vectorizer')
                self.classifiers = data.get('classifiers', {})
                self.symptom_weights = data.get('weights', self.symptom_weights)
                self.epoch_count = data.get('epoch_count', 0)
            return True
        return False


symptom_classifier = SymptomClassifier()
SYMPTOM_MODEL_FILE = os.path.join(MODELS_DIR, "symptom_classifier.pkl")


def normalize_text(text: str) -> str:
    text = text.lower().strip()
    text = re.sub(r'\s+', ' ', text)
    return text

def extract_crop(text: str, use_model: bool = True) -> Optional[str]:
    if use_model and (crop_classifier.model is not None or any(w != 1.0 for w in crop_classifier.keyword_weights.values())):
        pred = crop_classifier.predict(text)
        if pred:
            return pred
    normalized = normalize_text(text)
    for crop, keywords in CROP_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                return crop
    return None


def extract_stage(text: str) -> Optional[str]:
    normalized = normalize_text(text)
    for stage, keywords in STAGE_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                return stage
    return None


def extract_symptoms(text: str) -> List[str]:
    normalized = normalize_text(text)
    found = []
    for symptom, keywords in SYMPTOM_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                if symptom not in found:
                    found.append(symptom)
                break
    return found


def extract_region(text: str) -> str:
    normalized = normalize_text(text)
    for region, keywords in REGION_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                return region.value
    return Region.UNKNOWN.value


def extract_weather(text: str) -> Optional[str]:
    normalized = normalize_text(text)
    found = []
    for weather, keywords in WEATHER_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                found.append(weather)
                break
    return ", ".join(found) if found else None


def extract_season(text: str, weather_context: Optional[str]) -> str:
    normalized = normalize_text(text)
    indicators = {
        "Ä‘Ã´ng xuÃ¢n": Season.DONG_XUAN,
        "hÃ¨ thu": Season.HE_THU,
        "thu Ä‘Ã´ng": Season.THU_DONG,
        "mÃ¹a mÆ°a": Season.MUA,
        "mÃ¹a khÃ´": Season.KHO,
    }
    for ind, season in indicators.items():
        if ind in normalized:
            return season.value
    if weather_context:
        if "mÆ°a" in weather_context:
            return Season.MUA.value
        elif "náº¯ng" in weather_context or "khÃ´" in weather_context:
            return Season.KHO.value
    return Season.UNKNOWN.value


def extract_action_asked(text: str) -> Optional[str]:
    normalized = normalize_text(text)
    for action, keywords in ACTION_KEYWORDS.items():
        for keyword in keywords:
            if keyword in normalized:
                return action
    patterns = [
        (r"cÃ³ nÃªn (.+?) khÃ´ng", "há»i Ã½ kiáº¿n vá»"),
        (r"lÃ m sao (.+)", "há»i cÃ¡ch"),
        (r"pháº£i lÃ m gÃ¬", "há»i giáº£i phÃ¡p"),
        (r"táº¡i sao (.+)", "há»i nguyÃªn nhÃ¢n"),
        (r"vÃ¬ sao (.+)", "há»i nguyÃªn nhÃ¢n"),
    ]
    for pattern, action_type in patterns:
        if re.search(pattern, normalized):
            return action_type
    return None


def detect_urgency(text: str, symptoms: List[str]) -> str:
    normalized = normalize_text(text)
    urgent = ["cháº¿t", "hÃ©o rÅ©", "chÃ¡y háº¿t", "rá»¥ng sáº¡ch", "kháº©n cáº¥p", "gáº¥p", "nhanh", "ngay", "lan nhanh", "cáº£ ruá»™ng"]
    for kw in urgent:
        if kw in normalized:
            return "urgent"
    severe = ["thá»‘i rá»…", "cháº¿t cÃ¢y", "chÃ¡y lÃ¡", "virus"]
    for s in symptoms:
        if s in severe:
            return "high"
    if len(symptoms) >= 3:
        return "high"
    elif len(symptoms) >= 2:
        return "medium"
    return "normal"


def detect_experience_level(text: str) -> str:
    normalized = normalize_text(text)
    beginner = ["khÃ´ng biáº¿t", "láº§n Ä‘áº§u", "má»›i trá»“ng", "má»›i táº­p", "chÆ°a biáº¿t", "há»i thÄƒm", "nhá» chá»‰"]
    expert = ["thÆ°á»ng thÃ¬", "nÄƒm ngoÃ¡i", "máº¥y nÄƒm nay", "kinh nghiá»‡m", "Ä‘Ã£ thá»­", "Ä‘Ã£ bÃ³n", "vá»¥ trÆ°á»›c"]
    for ind in expert:
        if ind in normalized:
            return Experience.CO_KINH_NGHIEM.value
    for ind in beginner:
        if ind in normalized:
            return Experience.PHO_THONG.value
    return Experience.UNKNOWN.value


def detect_scale(text: str) -> str:
    normalized = normalize_text(text)
    farm = ["trang tráº¡i", "máº¥y hÃ©c", "hecta", "ha", "máº«u", "cÃ´ng ty"]
    home = ["nhÃ  tui", "vÆ°á»n nhÃ ", "sÃ¢n nhÃ ", "máº¥y gá»‘c", "vÃ i cÃ¢y", "Ã­t cÃ¢y"]
    for ind in farm:
        if ind in normalized:
            return Scale.TRANG_TRAI.value
    for ind in home:
        if ind in normalized:
            return Scale.NHA_VUON.value
    return Scale.UNKNOWN.value


def analyze_question(text: str, use_model: bool = True) -> QuestionAnalysis:
    weather_context = extract_weather(text)
    symptoms = extract_symptoms(text)
    return QuestionAnalysis(
        original_question=text,
        crop=extract_crop(text, use_model=use_model),
        stage=extract_stage(text),
        symptoms=symptoms,
        region=extract_region(text),
        season=extract_season(text, weather_context),
        scale=detect_scale(text),
        experience=detect_experience_level(text),
        weather_context=weather_context,
        time_context=None,
        action_asked=extract_action_asked(text),
        urgency_level=detect_urgency(text, symptoms),
    )


class AgriKnowledgeBase:
    def __init__(self):
        self.rules = self._init_rules()

    def _init_rules(self) -> List[Dict]:
        return [
            {
                "id": "LUA_001",
                "conditions": {"crop": "lÃºa", "stage": "Ä‘áº» nhÃ¡nh", "symptoms": ["vÃ ng lÃ¡ tá»« gá»‘c"], "weather": "mÆ°a"},
                "conclusions": {
                    "priority_causes": ["Ngháº¹t rá»… do ngáº­p Ãºng", "Thiáº¿u oxy vÃ¹ng rá»…"],
                    "secondary_causes": ["Thiáº¿u Ä‘áº¡m (N)", "Náº¥m bá»‡nh vÃ¹ng rá»…"],
                    "recommended_actions": ["ThÃ¡o bá»›t nÆ°á»›c ruá»™ng, Ä‘á»ƒ má»±c nÆ°á»›c 3-5cm", "Kiá»ƒm tra rá»… lÃºa (rá»… Ä‘en = ngháº¹t rá»…)", "Náº¿u rá»… tráº¯ng khá»e má»›i bÃ³n phÃ¢n"],
                    "avoid_actions": ["KHÃ”NG bÃ³n phÃ¢n Ä‘áº¡m ngay khi chÆ°a kiá»ƒm tra rá»…", "KHÃ”NG Ä‘á»ƒ ruá»™ng ngáº­p sÃ¢u quÃ¡ 10cm"],
                    "check_first": ["MÃ u sáº¯c rá»… lÃºa (tráº¯ng = khá»e, Ä‘en/nÃ¢u = ngháº¹t)", "Má»±c nÆ°á»›c ruá»™ng hiá»‡n táº¡i", "TÃ¬nh tráº¡ng thoÃ¡t nÆ°á»›c"]
                },
                "confidence": "high",
                "reasoning": "MÆ°a nhiá»u + vÃ ng lÃ¡ tá»« gá»‘c á»Ÿ giai Ä‘oáº¡n Ä‘áº» nhÃ¡nh thÆ°á»ng do ngháº¹t rá»…"
            },
            {
                "id": "LUA_002",
                "conditions": {"crop": "lÃºa", "symptoms": ["vÃ ng lÃ¡ tá»« ngá»n"]},
                "conclusions": {
                    "priority_causes": ["Thiáº¿u Ä‘áº¡m (N)"],
                    "secondary_causes": ["Äáº¥t nghÃ¨o dinh dÆ°á»¡ng", "Rá»… yáº¿u khÃ´ng hÃºt Ä‘Æ°á»£c dinh dÆ°á»¡ng"],
                    "recommended_actions": ["BÃ³n bá»• sung phÃ¢n Ä‘áº¡m (UrÃª) 3-5kg/1000mÂ²", "Káº¿t há»£p phÃ¢n bÃ³n lÃ¡ náº¿u cáº§n nhanh"],
                    "avoid_actions": ["KHÃ”NG bÃ³n quÃ¡ nhiá»u má»™t láº§n (dá»… chÃ¡y lÃ¡)", "KHÃ”NG bÃ³n khi trá»i náº¯ng gáº¯t"],
                    "check_first": ["XÃ¡c nháº­n vÃ ng tá»« ngá»n xuá»‘ng, khÃ´ng pháº£i tá»« gá»‘c lÃªn"]
                },
                "confidence": "medium",
                "reasoning": "VÃ ng tá»« ngá»n thÆ°á»ng lÃ  thiáº¿u Ä‘áº¡m"
            },
            {
                "id": "LUA_003",
                "conditions": {"crop": "lÃºa", "symptoms": ["ráº§y"]},
                "conclusions": {
                    "priority_causes": ["Ráº§y nÃ¢u táº¥n cÃ´ng"],
                    "secondary_causes": ["BÃ³n quÃ¡ nhiá»u Ä‘áº¡m", "Máº­t Ä‘á»™ sáº¡ quÃ¡ dÃ y"],
                    "recommended_actions": ["Kiá»ƒm tra máº­t Ä‘á»™ ráº§y", "Náº¿u >3 con/dáº£nh: phun thuá»‘c Ä‘áº·c trá»‹", "Thuá»‘c khuyáº¿n cÃ¡o: Bassa, Applaud, Chess"],
                    "avoid_actions": ["KHÃ”NG phun thuá»‘c bá»«a bÃ£i", "KHÃ”NG bÃ³n thÃªm Ä‘áº¡m khi cÃ³ ráº§y"],
                    "check_first": ["Äáº¿m máº­t Ä‘á»™ ráº§y thá»±c táº¿", "XÃ¡c Ä‘á»‹nh loáº¡i ráº§y"]
                },
                "confidence": "high",
                "reasoning": "Ráº§y lÃ  Ä‘á»‘i tÆ°á»£ng gÃ¢y háº¡i nghiÃªm trá»ng trÃªn lÃºa"
            },
            {
                "id": "CAPHE_001",
                "conditions": {"crop": "cÃ  phÃª", "symptoms": ["vÃ ng lÃ¡"], "weather": "mÆ°a"},
                "conclusions": {
                    "priority_causes": ["Thá»‘i rá»… do náº¥m Fusarium", "Ngáº­p Ãºng vÃ¹ng rá»…"],
                    "secondary_causes": ["Tuyáº¿n trÃ¹ng háº¡i rá»…", "Thiáº¿u vi lÆ°á»£ng"],
                    "recommended_actions": ["ÄÃ o rÃ£nh thoÃ¡t nÆ°á»›c quanh gá»‘c", "Kiá»ƒm tra rá»… cÃ  phÃª", "Xá»­ lÃ½ náº¥m bá»‡nh báº±ng thuá»‘c gá»‘c Ä‘á»“ng"],
                    "avoid_actions": ["KHÃ”NG tÆ°á»›i thÃªm nÆ°á»›c", "KHÃ”NG bÃ³n phÃ¢n hÃ³a há»c khi rá»… Ä‘ang yáº¿u"],
                    "check_first": ["TÃ¬nh tráº¡ng thoÃ¡t nÆ°á»›c vÆ°á»n", "MÃ u sáº¯c vÃ  mÃ¹i cá»§a rá»…"]
                },
                "confidence": "medium",
                "reasoning": "CÃ  phÃª ráº¥t nháº¡y cáº£m vá»›i ngáº­p Ãºng"
            },
            {
                "id": "CAPHE_002",
                "conditions": {"crop": "cÃ  phÃª", "stage": "ra hoa", "symptoms": ["rá»¥ng hoa"]},
                "conclusions": {
                    "priority_causes": ["Thiáº¿u nÆ°á»›c giai Ä‘oáº¡n ra hoa", "Thá»i tiáº¿t báº¥t lá»£i"],
                    "secondary_causes": ["Thiáº¿u Bo (B)", "SÃ¢u Ä‘á»¥c hoa"],
                    "recommended_actions": ["TÆ°á»›i Ä‘á»§ nÆ°á»›c, duy trÃ¬ Ä‘á»™ áº©m Ä‘áº¥t 60-70%", "Phun phÃ¢n bÃ³n lÃ¡ cÃ³ Bo"],
                    "avoid_actions": ["KHÃ”NG Ä‘á»ƒ cÃ¢y khÃ´ háº¡n", "KHÃ”NG phun thuá»‘c cÃ³ mÃ¹i ná»“ng"],
                    "check_first": ["Äá»™ áº©m Ä‘áº¥t vÃ¹ng rá»…", "CÃ³ ong Ä‘áº¿n thá»¥ pháº¥n khÃ´ng"]
                },
                "confidence": "medium",
                "reasoning": "Giai Ä‘oáº¡n ra hoa cÃ  phÃª ráº¥t nháº¡y cáº£m"
            },
            {
                "id": "RAU_001",
                "conditions": {"crop": "rau", "symptoms": ["sÃ¢u"]},
                "conclusions": {
                    "priority_causes": ["SÃ¢u Äƒn lÃ¡ (sÃ¢u xanh, sÃ¢u tÆ¡)"],
                    "secondary_causes": ["Máº­t Ä‘á»™ trá»“ng quÃ¡ dÃ y", "Thiáº¿u thiÃªn Ä‘á»‹ch"],
                    "recommended_actions": ["Báº¯t sÃ¢u báº±ng tay náº¿u Ã­t", "DÃ¹ng thuá»‘c sinh há»c (BT, NPV)", "LuÃ¢n canh cÃ¢y trá»“ng"],
                    "avoid_actions": ["KHÃ”NG dÃ¹ng thuá»‘c hÃ³a há»c máº¡nh gáº§n thu hoáº¡ch"],
                    "check_first": ["XÃ¡c Ä‘á»‹nh loáº¡i sÃ¢u cá»¥ thá»ƒ", "Thá»i gian cÃ²n láº¡i Ä‘áº¿n thu hoáº¡ch"]
                },
                "confidence": "high",
                "reasoning": "Rau cáº§n Ä‘áº£m báº£o an toÃ n thá»±c pháº©m"
            },
            {
                "id": "GENERAL_001",
                "conditions": {"symptoms": ["thá»‘i rá»…"]},
                "conclusions": {
                    "priority_causes": ["Náº¥m bá»‡nh vÃ¹ng rá»…", "Ngáº­p Ãºng kÃ©o dÃ i"],
                    "secondary_causes": ["BÃ³n quÃ¡ nhiá»u phÃ¢n", "Äáº¥t nÃ©n cháº·t thiáº¿u oxy"],
                    "recommended_actions": ["Cáº£i thiá»‡n thoÃ¡t nÆ°á»›c ngay", "Xá»­ lÃ½ náº¥m báº±ng Trichoderma hoáº·c thuá»‘c gá»‘c Ä‘á»“ng"],
                    "avoid_actions": ["KHÃ”NG bÃ³n phÃ¢n hÃ³a há»c khi rá»… Ä‘ang thá»‘i", "KHÃ”NG tÆ°á»›i ngáº­p"],
                    "check_first": ["Má»©c Ä‘á»™ thá»‘i rá»…", "CÃ³ thá»ƒ cá»©u Ä‘Æ°á»£c khÃ´ng"]
                },
                "confidence": "high",
                "reasoning": "Thá»‘i rá»… lÃ  váº¥n Ä‘á» nghiÃªm trá»ng"
            },
            {
                "id": "GENERAL_002",
                "conditions": {"symptoms": ["náº¥m"], "weather": "áº©m"},
                "conclusions": {
                    "priority_causes": ["Náº¥m bá»‡nh do Ä‘á»™ áº©m cao"],
                    "secondary_causes": ["ThÃ´ng giÃ³ kÃ©m", "Máº­t Ä‘á»™ trá»“ng dÃ y"],
                    "recommended_actions": ["Tá»‰a bá»›t lÃ¡, cÃ nh Ä‘á»ƒ thÃ´ng thoÃ¡ng", "Phun thuá»‘c trá»« náº¥m"],
                    "avoid_actions": ["KHÃ”NG tÆ°á»›i phun lÃªn lÃ¡", "KHÃ”NG bÃ³n phÃ¢n Ä‘áº¡m cao"],
                    "check_first": ["Loáº¡i náº¥m bá»‡nh cá»¥ thá»ƒ", "Má»©c Ä‘á»™ lan rá»™ng"]
                },
                "confidence": "medium",
                "reasoning": "MÃ´i trÆ°á»ng áº©m Æ°á»›t táº¡o Ä‘iá»u kiá»‡n cho náº¥m bá»‡nh"
            },
        ]

    def find_matching_rules(self, analysis: QuestionAnalysis) -> List[Dict]:
        matching = []
        for rule in self.rules:
            cond = rule["conditions"]
            if "crop" in cond:
                if analysis.crop is None:
                    continue
                if cond["crop"].lower() != analysis.crop.lower():
                    continue
            score = 0
            total = len(cond)
            if "crop" in cond:
                score += 1
            if "stage" in cond:
                if analysis.stage and cond["stage"].lower() in analysis.stage.lower():
                    score += 1
                elif analysis.stage is None:
                    score += 0.2
            if "symptoms" in cond:
                matched = sum(1 for s in cond["symptoms"] if any(s in sym for sym in analysis.symptoms))
                if matched > 0:
                    score += matched / len(cond["symptoms"])
            if "weather" in cond:
                if analysis.weather_context and cond["weather"].lower() in analysis.weather_context.lower():
                    score += 1
                elif analysis.weather_context is None:
                    score += 0.2
            ratio = score / total if total > 0 else 0
            if ratio >= 0.5:
                matching.append({"rule": rule, "match_score": ratio})
        matching.sort(key=lambda x: x["match_score"], reverse=True)
        return matching


def apply_agri_logic(analysis: QuestionAnalysis) -> AgriLogicResult:
    kb = AgriKnowledgeBase()
    matches = kb.find_matching_rules(analysis)
    priority_causes, secondary_causes, recommended_actions, avoid_actions, check_first = [], [], [], [], []
    knowledge_notes, reasoning_chain = [], []
    if matches:
        for m in matches[:3]:
            rule = m["rule"]
            score = m["match_score"]
            conc = rule["conclusions"]
            reasoning_chain.append(f"Ãp dá»¥ng rule {rule['id']} (Ä‘á»™ khá»›p: {score:.0%}): {rule['reasoning']}")
            priority_causes.extend(conc.get("priority_causes", []))
            secondary_causes.extend(conc.get("secondary_causes", []))
            recommended_actions.extend(conc.get("recommended_actions", []))
            avoid_actions.extend(conc.get("avoid_actions", []))
            check_first.extend(conc.get("check_first", []))
        confidence = "high" if matches[0]["match_score"] >= 0.8 else "medium"
    else:
        reasoning_chain.append("KhÃ´ng tÃ¬m tháº¥y rule phÃ¹ há»£p, Ä‘Æ°a ra khuyáº¿n nghá»‹ chung")
        if analysis.symptoms:
            priority_causes.append(f"Cáº§n kiá»ƒm tra thÃªm vá»: {', '.join(analysis.symptoms)}")
        recommended_actions.append("Quan sÃ¡t thÃªm vÃ  mÃ´ táº£ chi tiáº¿t hÆ¡n")
        check_first.append("XÃ¡c Ä‘á»‹nh rÃµ triá»‡u chá»©ng vÃ  giai Ä‘oáº¡n cÃ¢y")
        confidence = "low"
    priority_causes = list(dict.fromkeys(priority_causes))
    secondary_causes = list(dict.fromkeys(secondary_causes))
    recommended_actions = list(dict.fromkeys(recommended_actions))
    avoid_actions = list(dict.fromkeys(avoid_actions))
    check_first = list(dict.fromkeys(check_first))
    if analysis.crop:
        knowledge_notes.append(f"Loáº¡i cÃ¢y: {analysis.crop}")
    if analysis.stage:
        knowledge_notes.append(f"Giai Ä‘oáº¡n: {analysis.stage}")
    if analysis.weather_context:
        knowledge_notes.append(f"Thá»i tiáº¿t: {analysis.weather_context}")
    if analysis.region != Region.UNKNOWN.value:
        knowledge_notes.append(f"VÃ¹ng miá»n: {analysis.region}")
    return AgriLogicResult(
        priority_causes=priority_causes,
        secondary_causes=secondary_causes,
        recommended_actions=recommended_actions,
        avoid_actions=avoid_actions,
        check_first=check_first,
        knowledge_notes=knowledge_notes,
        confidence_level=confidence,
        reasoning_chain=reasoning_chain,
    )


def build_prompt(analysis: QuestionAnalysis, logic_result: AgriLogicResult, mode: str = "runtime") -> str:
    if mode == "debug":
        return _build_prompt_debug(analysis, logic_result)
    return _build_prompt_runtime(analysis, logic_result)


def _build_prompt_debug(analysis: QuestionAnalysis, logic_result: AgriLogicResult) -> str:
    parts = []
    parts.append("""=== VAI TRÃ’ ===
Báº¡n lÃ  má»™t Ká»¸ SÆ¯ NÃ”NG NGHIá»†P VIá»†T NAM giÃ u kinh nghiá»‡m, chuyÃªn tÆ° váº¥n cho nÃ´ng dÃ¢n.
- Báº¡n hiá»ƒu rÃµ Ä‘iá»u kiá»‡n canh tÃ¡c, khÃ­ háº­u, vÃ  thá»±c tiá»…n nÃ´ng nghiá»‡p Viá»‡t Nam
- Báº¡n nÃ³i chuyá»‡n thÃ¢n thiá»‡n, dá»… hiá»ƒu, dÃ¹ng ngÃ´n ngá»¯ Ä‘á»i thÆ°á»ng
- Báº¡n KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin, náº¿u khÃ´ng cháº¯c cháº¯n sáº½ nÃ³i rÃµ
- Báº¡n Æ°u tiÃªn cÃ¡c biá»‡n phÃ¡p an toÃ n, tiáº¿t kiá»‡m, hiá»‡u quáº£""")
    parts.append(f'\n=== CÃ‚U Há»ŽI Gá»C Cá»¦A NÃ”NG DÃ‚N ===\n"{analysis.original_question}"\n')
    ctx = ["=== Bá»I Cáº¢NH ÄÃƒ PHÃ‚N TÃCH ==="]
    ctx.append(f"â€¢ Loáº¡i cÃ¢y trá»“ng: {analysis.crop or 'ChÆ°a xÃ¡c Ä‘á»‹nh rÃµ'}")
    ctx.append(f"â€¢ Giai Ä‘oáº¡n sinh trÆ°á»Ÿng: {analysis.stage or 'ChÆ°a xÃ¡c Ä‘á»‹nh'}")
    ctx.append(f"â€¢ Triá»‡u chá»©ng phÃ¡t hiá»‡n: {', '.join(analysis.symptoms) if analysis.symptoms else 'KhÃ´ng mÃ´ táº£ rÃµ'}")
    if analysis.weather_context:
        ctx.append(f"â€¢ Äiá»u kiá»‡n thá»i tiáº¿t: {analysis.weather_context}")
    if analysis.region != Region.UNKNOWN.value:
        ctx.append(f"â€¢ VÃ¹ng miá»n: {analysis.region}")
    if analysis.season != Season.UNKNOWN.value:
        ctx.append(f"â€¢ MÃ¹a vá»¥: {analysis.season}")
    if analysis.scale != Scale.UNKNOWN.value:
        ctx.append(f"â€¢ Quy mÃ´: {analysis.scale}")
    if analysis.action_asked:
        ctx.append(f"â€¢ NÃ´ng dÃ¢n Ä‘ang há»i vá»: {analysis.action_asked}")
    ctx.append(f"â€¢ Má»©c Ä‘á»™ kháº©n cáº¥p: {analysis.urgency_level}")
    parts.append("\n".join(ctx))
    if logic_result.priority_causes or logic_result.recommended_actions:
        sys_analysis = ["", "=== NHáº¬N Äá»ŠNH BAN Äáº¦U Cá»¦A Há»† THá»NG ==="]
        sys_analysis.append(f"(Äá»™ tin cáº­y: {logic_result.confidence_level})")
        if logic_result.reasoning_chain:
            sys_analysis.append("")
            sys_analysis.append("Chuá»—i suy luáº­n:")
            for i, r in enumerate(logic_result.reasoning_chain, 1):
                sys_analysis.append(f"  {i}. {r}")
        if logic_result.priority_causes:
            sys_analysis.append("")
            sys_analysis.append("NguyÃªn nhÃ¢n cÃ³ kháº£ nÄƒng cao nháº¥t:")
            for c in logic_result.priority_causes:
                sys_analysis.append(f"  âž¤ {c}")
        if logic_result.secondary_causes:
            sys_analysis.append("")
            sys_analysis.append("NguyÃªn nhÃ¢n phá»¥ cáº§n xem xÃ©t:")
            for c in logic_result.secondary_causes:
                sys_analysis.append(f"  â€¢ {c}")
        if logic_result.check_first:
            sys_analysis.append("")
            sys_analysis.append("Cáº§n kiá»ƒm tra trÆ°á»›c:")
            for c in logic_result.check_first:
                sys_analysis.append(f"  âœ“ {c}")
        if logic_result.recommended_actions:
            sys_analysis.append("")
            sys_analysis.append("Khuyáº¿n nghá»‹ hÃ nh Ä‘á»™ng:")
            for a in logic_result.recommended_actions:
                sys_analysis.append(f"  â†’ {a}")
        if logic_result.avoid_actions:
            sys_analysis.append("")
            sys_analysis.append("âš ï¸ TRÃNH LÃ€M:")
            for a in logic_result.avoid_actions:
                sys_analysis.append(f"  âœ— {a}")
        parts.append("\n".join(sys_analysis))
    parts.append("""
=== HÆ¯á»šNG DáºªN TRáº¢ Lá»œI ===
1. Dá»±a trÃªn phÃ¢n tÃ­ch trÃªn, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a nÃ´ng dÃ¢n má»™t cÃ¡ch:
   - ThÃ¢n thiá»‡n, dá»… hiá»ƒu (trÃ¡nh thuáº­t ngá»¯ quÃ¡ chuyÃªn mÃ´n)
   - Cá»¥ thá»ƒ, cÃ³ thá»ƒ Ã¡p dá»¥ng ngay
   - Trung thá»±c (náº¿u chÆ°a cháº¯c cháº¯n, hÃ£y nÃ³i rÃµ cáº§n kiá»ƒm tra thÃªm)
2. Cáº¥u trÃºc cÃ¢u tráº£ lá»i:
   - Báº¯t Ä‘áº§u báº±ng viá»‡c thÃ´ng cáº£m/hiá»ƒu váº¥n Ä‘á» cá»§a nÃ´ng dÃ¢n
   - Giáº£i thÃ­ch ngáº¯n gá»n nguyÃªn nhÃ¢n cÃ³ thá»ƒ
   - ÄÆ°a ra hÆ°á»›ng dáº«n cá»¥ thá»ƒ, tá»«ng bÆ°á»›c
   - Káº¿t thÃºc báº±ng lá»i khuyÃªn theo dÃµi hoáº·c phÃ²ng ngá»«a
3. LÆ°u Ã½ quan trá»ng:
   - Æ¯u tiÃªn kiá»ƒm tra trÆ°á»›c khi hÃ nh Ä‘á»™ng (Ä‘áº·c biá»‡t vá»›i bÃ³n phÃ¢n, phun thuá»‘c)
   - Äá» cáº­p Ä‘áº¿n viá»‡c Cáº¦N TRÃNH náº¿u cÃ³
   - Náº¿u tÃ¬nh huá»‘ng nghiÃªm trá»ng, khuyÃªn liÃªn há»‡ cÃ¡n bá»™ khuyáº¿n nÃ´ng Ä‘á»‹a phÆ°Æ¡ng
4. Sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ lÃ m rÃµ Ã½:
   ðŸŒ± cho cÃ¢y trá»“ng | ðŸ’§ cho nÆ°á»›c/tÆ°á»›i | â˜€ï¸ cho thá»i tiáº¿t
   âš ï¸ cho cáº£nh bÃ¡o | âœ… cho khuyáº¿n nghá»‹ | âŒ cho trÃ¡nh lÃ m""")
    return "\n".join(parts)


def _build_prompt_runtime(analysis: QuestionAnalysis, logic_result: AgriLogicResult) -> str:
    lines = []
    lines.append("Báº¡n lÃ  ká»¹ sÆ° nÃ´ng nghiá»‡p VN, tÆ° váº¥n thÃ¢n thiá»‡n, dá»… hiá»ƒu, khÃ´ng bá»‹a Ä‘áº·t.")
    lines.append(f'CÃ¢u há»i: "{analysis.original_question}"')
    ctx_parts = []
    if analysis.crop:
        ctx_parts.append(f"CÃ¢y: {analysis.crop}")
    if analysis.stage:
        ctx_parts.append(f"Giai Ä‘oáº¡n: {analysis.stage}")
    if analysis.symptoms:
        ctx_parts.append(f"Triá»‡u chá»©ng: {', '.join(analysis.symptoms[:3])}")
    if analysis.weather_context:
        ctx_parts.append(f"Thá»i tiáº¿t: {analysis.weather_context}")
    if ctx_parts:
        lines.append("Bá»‘i cáº£nh: " + "; ".join(ctx_parts))
    if logic_result.priority_causes:
        lines.append("NguyÃªn nhÃ¢n chÃ­nh: " + "; ".join(logic_result.priority_causes[:3]))
    if logic_result.check_first:
        lines.append("Kiá»ƒm tra trÆ°á»›c: " + "; ".join(logic_result.check_first[:3]))
    if logic_result.recommended_actions:
        lines.append("Khuyáº¿n nghá»‹: " + "; ".join(logic_result.recommended_actions[:3]))
    if logic_result.avoid_actions:
        lines.append("TrÃ¡nh: " + "; ".join(logic_result.avoid_actions[:2]))
    lines.append("Tráº£ lá»i ngáº¯n gá»n, cá»¥ thá»ƒ, dÃ¹ng emoji ðŸŒ±ðŸ’§âš ï¸âœ…âŒ phÃ¹ há»£p.")
    prompt = "\n".join(lines)
    if len(prompt) > 800:
        prompt = prompt[:797] + "..."
    return prompt


def confidence_to_numeric(conf: str) -> float:
    return {"high": 0.9, "medium": 0.6, "low": 0.3}.get(conf, 0.3)


def compute_friendliness(prompt: str) -> float:
    sentences = re.split(r'[.!?ã€‚\n]+', prompt)
    sentences = [s.strip() for s in sentences if s.strip()]
    if not sentences:
        return 1.0
    total_words = sum(len(s.split()) for s in sentences)
    mean_len = total_words / len(sentences)
    return max(0.0, min(1.0, 1.0 - (mean_len / 30.0)))


def evaluate_prediction(gold: Dict, pred_analysis: QuestionAnalysis, logic_result: AgriLogicResult, prompt: str) -> Dict:
    gold_crop = gold.get("crop", "").lower() if gold.get("crop") else ""
    pred_crop = (pred_analysis.crop or "").lower()
    crop_match = 1 if gold_crop and gold_crop == pred_crop else 0
    gold_symptoms = [s.lower() for s in gold.get("symptoms", [])]
    pred_symptoms = [s.lower() for s in pred_analysis.symptoms]
    if gold_symptoms:
        matched = sum(1 for gs in gold_symptoms if any(gs in ps or ps in gs for ps in pred_symptoms))
        symptom_match = matched / len(gold_symptoms)
    else:
        symptom_match = 1.0 if not pred_symptoms else 0.5
    conf_num = confidence_to_numeric(logic_result.confidence_level)
    friend_num = compute_friendliness(prompt)
    return {"crop_match": crop_match, "symptom_match": symptom_match, "confidence": conf_num, "friendliness": friend_num}


def compute_metrics(results: List[Dict]) -> Dict:
    if not results:
        return {"accuracy_overall": 0.0, "confidence_avg": 0.0, "friendliness": 0.0}
    crop_accs = [r["crop_match"] for r in results]
    symptom_accs = [r["symptom_match"] for r in results]
    confs = [r["confidence"] for r in results]
    friends = [r["friendliness"] for r in results]
    crop_acc = sum(crop_accs) / len(crop_accs)
    symptom_acc = sum(symptom_accs) / len(symptom_accs)
    accuracy_overall = (crop_acc + symptom_acc) / 2
    confidence_avg = sum(confs) / len(confs)
    friendliness_avg = sum(friends) / len(friends)
    return {"accuracy_overall": accuracy_overall, "confidence_avg": confidence_avg, "friendliness": friendliness_avg}


DEFAULT_TRAIN_SAMPLES = [
    {"question": "LÃºa nhÃ  tui Ä‘ang Ä‘áº» nhÃ¡nh mÃ  vÃ ng lÃ¡ tá»« gá»‘c, mÆ°a nhiá»u, cÃ³ nÃªn bÃ³n phÃ¢n khÃ´ng?", "labels": {"crop": "lÃºa", "symptoms": ["vÃ ng lÃ¡ tá»« gá»‘c"]}},
    {"question": "CÃ  phÃª bá»‹ vÃ ng lÃ¡, rá»¥ng nhiá»u sau mÆ°a, nÃªn lÃ m sao?", "labels": {"crop": "cÃ  phÃª", "symptoms": ["vÃ ng lÃ¡"]}},
    {"question": "TiÃªu nhÃ  em bá»‹ thá»‘i rá»…, lÃ¡ hÃ©o dáº§n, cÃ³ cÃ¡ch nÃ o cá»©u khÃ´ng?", "labels": {"crop": "tiÃªu", "symptoms": ["thá»‘i rá»…", "hÃ©o"]}},
    {"question": "Ráº§y nÃ¢u nhiá»u quÃ¡, Ä‘áº¿m cáº£ chá»¥c con trÃªn bá»¥i lÃºa, phun thuá»‘c gÃ¬ áº¡?", "labels": {"crop": "lÃºa", "symptoms": ["ráº§y"]}},
    {"question": "Sáº§u riÃªng Ä‘ang ra hoa mÃ  mÆ°a hoÃ i, sá»£ khÃ´ng Ä‘áº­u trÃ¡i, lÃ m sao?", "labels": {"crop": "sáº§u riÃªng", "symptoms": []}},
    {"question": "CÃ¢y cam bá»‹ Ä‘á»‘m lÃ¡, lÃ¡ vÃ ng rá»“i rá»¥ng dáº§n, cÃ³ pháº£i náº¥m khÃ´ng?", "labels": {"crop": "cam", "symptoms": ["Ä‘á»‘m lÃ¡", "vÃ ng lÃ¡", "rá»¥ng lÃ¡"]}},
    {"question": "NgÃ´ nhÃ  tÃ´i lÃ¡ xoÄƒn láº¡i, cÃ²i cá»c khÃ´ng lá»›n, thiáº¿u gÃ¬ váº­y?", "labels": {"crop": "ngÃ´", "symptoms": ["xoÄƒn lÃ¡", "cháº­m lá»›n"]}},
    {"question": "Rau cáº£i bá»‹ sÃ¢u Äƒn lÃ¡ nhiá»u quÃ¡, gáº§n thu hoáº¡ch rá»“i, xá»­ lÃ½ sao?", "labels": {"crop": "rau cáº£i", "symptoms": ["sÃ¢u"]}},
]


def create_train_file():
    ensure_directories()
    if not os.path.exists(TRAIN_FILE):
        with open(TRAIN_FILE, 'w', encoding='utf-8') as f:
            for sample in DEFAULT_TRAIN_SAMPLES:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")
        print(f"âœ… ÄÃ£ táº¡o file huáº¥n luyá»‡n máº«u: {TRAIN_FILE}")


def load_train_data() -> List[Dict]:
    create_train_file()
    data = []
    with open(TRAIN_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                data.append(json.loads(line))
    return data


def run_train_mode(epochs: int = 50):
    """
    Advanced Training Mode with:
    - Train/Validation split (80/20)
    - Dual classifier training (Crop + Symptoms)
    - Learning rate scheduling with warmup
    - Mini-batch training
    - Early stopping with patience
    - Comprehensive metrics tracking
    """
    ensure_directories()
    print("=" * 70)
    print("ðŸšœ AGRISENSE AI - ADVANCED TRAINING MODE")
    print(f"   Epochs: {epochs} | Seed: {RANDOM_SEED}")
    print("=" * 70)
    
    # Load data
    train_data = load_train_data()
    print(f"ðŸ“Š Loaded {len(train_data)} samples from {TRAIN_FILE}")
    
    # Shuffle and split train/validation (80/20)
    random.shuffle(train_data)
    split_idx = int(len(train_data) * 0.8)
    train_set = train_data[:split_idx]
    val_set = train_data[split_idx:]
    print(f"   Train set: {len(train_set)} | Validation set: {len(val_set)}")
    
    # Reset classifiers for fresh training
    global crop_classifier, symptom_classifier
    crop_classifier = CropClassifier()
    symptom_classifier = SymptomClassifier()
    
    # Training hyperparameters
    initial_lr = 0.2
    min_lr = 0.001
    warmup_epochs = 3
    lr_decay = 0.92
    patience = 15
    batch_size = min(32, len(train_set) // 4)
    
    best_combined_score = 0.0
    no_improve_count = 0
    
    all_metrics = []
    
    # Prepare data
    train_texts = [s["question"] for s in train_set]
    train_crop_labels = [s["labels"]["crop"] for s in train_set]
    train_symptom_labels = [s["labels"].get("symptoms", []) for s in train_set]
    
    val_texts = [s["question"] for s in val_set]
    val_crop_labels = [s["labels"]["crop"] for s in val_set]
    val_symptom_labels = [s["labels"].get("symptoms", []) for s in val_set]
    
    print(f"\nðŸŽ¯ Training Configuration:")
    print(f"   â€¢ Batch size: {batch_size}")
    print(f"   â€¢ Initial LR: {initial_lr}")
    print(f"   â€¢ LR decay: {lr_decay}")
    print(f"   â€¢ Warmup epochs: {warmup_epochs}")
    print(f"   â€¢ Patience: {patience}")
    
    print("\nðŸ“ˆ Training Progress:")
    print("-" * 90)
    print(f"{'Epoch':>5} | {'Loss':>7} | {'Crop':>6} | {'Symp':>6} | {'Val':>6} | {'F1':>6} | {'LR':>8} | {'Status':<12}")
    print("-" * 90)
    
    for epoch in range(1, epochs + 1):
        # Learning rate with warmup and decay
        if epoch <= warmup_epochs:
            current_lr = initial_lr * (epoch / warmup_epochs)
        else:
            current_lr = max(min_lr, initial_lr * (lr_decay ** (epoch - warmup_epochs)))
        
        # Shuffle and create mini-batches
        indices = list(range(len(train_texts)))
        random.shuffle(indices)
        
        epoch_loss = 0.0
        num_batches = 0
        
        for i in range(0, len(indices), batch_size):
            batch_indices = indices[i:i+batch_size]
            batch_texts = [train_texts[j] for j in batch_indices]
            batch_crop_labels = [train_crop_labels[j] for j in batch_indices]
            batch_symptom_labels = [train_symptom_labels[j] for j in batch_indices]
            
            # Train crop classifier
            crop_classifier.partial_fit(batch_texts, batch_crop_labels, learning_rate=current_lr)
            
            # Train symptom classifier
            symptom_classifier.partial_fit(batch_texts, batch_symptom_labels, learning_rate=current_lr)
            
            epoch_loss += crop_classifier.get_loss()
            num_batches += 1
        
        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
        
        # Evaluate on training set
        train_crop_eval = crop_classifier.evaluate(train_texts, train_crop_labels)
        train_crop_acc = train_crop_eval["accuracy"]
        
        train_symptom_eval = symptom_classifier.evaluate(train_texts, train_symptom_labels)
        train_symptom_f1 = train_symptom_eval["f1"]
        
        # Evaluate on validation set
        val_correct = 0
        val_results = []
        
        for text, crop_label, symptom_label in zip(val_texts, val_crop_labels, val_symptom_labels):
            analysis = analyze_question(text, use_model=True)
            logic = apply_agri_logic(analysis)
            prompt = build_prompt(analysis, logic, mode="runtime")
            
            # Check crop prediction
            pred_crop = (analysis.crop or "").lower()
            gold_crop = crop_label.lower() if crop_label else ""
            if pred_crop == gold_crop:
                val_correct += 1
            
            eval_result = evaluate_prediction(
                {"crop": crop_label, "symptoms": symptom_label}, 
                analysis, 
                logic, 
                prompt
            )
            val_results.append(eval_result)
        
        val_crop_acc = val_correct / len(val_set) if val_set else 0.0
        
        # Symptom validation
        val_symptom_eval = symptom_classifier.evaluate(val_texts, val_symptom_labels)
        val_symptom_f1 = val_symptom_eval["f1"]
        
        val_metrics = compute_metrics(val_results)
        
        # Combined score (weighted average)
        combined_score = 0.6 * val_crop_acc + 0.4 * val_symptom_f1
        
        # Determine status
        status = ""
        if combined_score > best_combined_score + 0.001:  # Small threshold to avoid noise
            best_combined_score = combined_score
            no_improve_count = 0
            status = "âœ¨ BEST"
            # Save best models
            crop_classifier.save(MODEL_FILE)
            symptom_classifier.save(SYMPTOM_MODEL_FILE)
        else:
            no_improve_count += 1
            if no_improve_count >= patience:
                status = "â¹ï¸ STOP"
            elif no_improve_count >= patience // 2:
                status = "âš ï¸ PLATEAU"
            else:
                status = ""
        
        # Store metrics
        epoch_metrics = {
            "epoch": epoch,
            "train_loss": avg_loss,
            "train_crop_acc": train_crop_acc,
            "train_symptom_f1": train_symptom_f1,
            "val_crop_acc": val_crop_acc,
            "val_symptom_f1": val_symptom_f1,
            "val_combined": combined_score,
            "val_accuracy_overall": val_metrics["accuracy_overall"],
            "confidence_avg": val_metrics["confidence_avg"],
            "friendliness": val_metrics["friendliness"],
            "learning_rate": current_lr,
            "best_combined_score": best_combined_score
        }
        all_metrics.append(epoch_metrics)
        
        # Print progress
        print(f"{epoch:>5} | {avg_loss:>7.4f} | {train_crop_acc*100:>5.1f}% | {train_symptom_f1*100:>5.1f}% | {val_crop_acc*100:>5.1f}% | {val_symptom_f1*100:>5.1f}% | {current_lr:>8.5f} | {status:<12}")
        
        # Early stopping
        if no_improve_count >= patience:
            print(f"\nâ¹ï¸ Early stopping at epoch {epoch} (no improvement for {patience} epochs)")
            break
    
    print("-" * 90)
    
    # Save final metrics
    with open(TRAIN_METRICS_LOG, 'w', encoding='utf-8') as f:
        json.dump(all_metrics, f, ensure_ascii=False, indent=2)
    print(f"\nðŸ“ˆ Metrics saved to {TRAIN_METRICS_LOG}")
    
    # Training summary
    print("\n" + "=" * 70)
    print("âœ… TRAINING COMPLETE")
    print("=" * 70)
    
    final = all_metrics[-1]
    best_epoch = max(all_metrics, key=lambda x: x["val_combined"])
    
    print(f"\nðŸ“Š FINAL RESULTS:")
    print(f"   â€¢ Total Epochs Run: {len(all_metrics)}")
    print(f"   â€¢ Best Combined Score: {best_combined_score*100:.1f}% (epoch {best_epoch['epoch']})")
    print(f"   â€¢ Best Val Crop Accuracy: {best_epoch['val_crop_acc']*100:.1f}%")
    print(f"   â€¢ Best Val Symptom F1: {best_epoch['val_symptom_f1']*100:.1f}%")
    print(f"   â€¢ Final Train Loss: {final['train_loss']:.4f}")
    print(f"   â€¢ Final Confidence: {final['confidence_avg']*100:.1f}%")
    print(f"   â€¢ Final Friendliness: {final['friendliness']*100:.1f}%")
    
    # Show training curve summary
    if len(all_metrics) >= 5:
        print(f"\nðŸ“ˆ TRAINING CURVE:")
        step = max(1, len(all_metrics) // 5)
        for i in range(0, len(all_metrics), step):
            m = all_metrics[i]
            bar_len = int(m["val_combined"] * 20)
            bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
            print(f"   Epoch {m['epoch']:3d}: [{bar}] Crop:{m['val_crop_acc']*100:5.1f}% Symp:{m['val_symptom_f1']*100:5.1f}%")
        # Always show last epoch
        if len(all_metrics) % step != 0:
            m = all_metrics[-1]
            bar_len = int(m["val_combined"] * 20)
            bar = "â–ˆ" * bar_len + "â–‘" * (20 - bar_len)
            print(f"   Epoch {m['epoch']:3d}: [{bar}] Crop:{m['val_crop_acc']*100:5.1f}% Symp:{m['val_symptom_f1']*100:5.1f}%")
    
    print(f"\nðŸ’¾ Models saved to:")
    print(f"   â€¢ Crop classifier: {MODEL_FILE}")
    print(f"   â€¢ Symptom classifier: {SYMPTOM_MODEL_FILE}")
    print("=" * 70)


def save_conversation(question: str, analysis: QuestionAnalysis, logic: AgriLogicResult, prompt: str):
    ensure_directories()
    record = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "analysis": analysis.to_dict(),
        "logic": logic.to_dict(),
        "prompt": prompt
    }
    with open(CONVERSATIONS_LOG, 'a', encoding='utf-8') as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")
    print(f"ðŸ’¾ ÄÃ£ lÆ°u vÃ o {CONVERSATIONS_LOG}")


def run_interactive_mode():
    ensure_directories()
    if crop_classifier.load(MODEL_FILE):
        print(f"ðŸ“¦ Loaded crop model from {MODEL_FILE}")
    if symptom_classifier.load(SYMPTOM_MODEL_FILE):
        print(f"ðŸ“¦ Loaded symptom model from {SYMPTOM_MODEL_FILE}")
    print("=" * 70)
    print("ðŸŒ¾ AGRISENSE AI - INTERACTIVE MODE")
    print("   Nháº­p cÃ¢u há»i nÃ´ng nghiá»‡p Ä‘á»ƒ phÃ¢n tÃ­ch")
    print("   GÃµ 'exit' hoáº·c 'quit' Ä‘á»ƒ thoÃ¡t")
    print("=" * 70)
    while True:
        print()
        try:
            question = input("ðŸ“ CÃ¢u há»i: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nðŸ‘‹ Táº¡m biá»‡t!")
            break
        if not question:
            continue
        if question.lower() in ("exit", "quit"):
            print("ðŸ‘‹ Táº¡m biá»‡t!")
            break
        analysis = analyze_question(question, use_model=True)
        logic = apply_agri_logic(analysis)
        prompt_runtime = build_prompt(analysis, logic, mode="runtime")
        prompt_debug = build_prompt(analysis, logic, mode="debug")
        print("\n" + "-" * 50)
        print("ðŸ” Káº¾T QUáº¢ PHÃ‚N TÃCH:")
        print(json.dumps(analysis.to_dict(), ensure_ascii=False, indent=2))
        print("\n" + "-" * 50)
        print("ðŸ§  LOGIC RESULT:")
        print(json.dumps(logic.to_dict(), ensure_ascii=False, indent=2))
        print("\n" + "-" * 50)
        print("ðŸ“¤ PROMPT (RUNTIME - rÃºt gá»n):")
        print(prompt_runtime)
        print("\n" + "-" * 50)
        prompt_len = len(prompt_runtime)
        token_est = prompt_len // 4
        print(f"ðŸ“Š Prompt length: {prompt_len} chars | ~{token_est} tokens")
        try:
            save_choice = input("\nðŸ’¾ LÆ°u káº¿t quáº£? (y/N): ").strip().lower()
        except (EOFError, KeyboardInterrupt):
            print("\nðŸ‘‹ Táº¡m biá»‡t!")
            break
        if save_choice == 'y':
            save_conversation(question, analysis, logic, prompt_runtime)


def main():
    parser = argparse.ArgumentParser(description="AgriSense AI - Agricultural Question Analysis Pipeline")
    parser.add_argument("--mode", choices=["interactive", "train"], default="interactive",
                        help="Mode: 'interactive' (default) or 'train'")
    parser.add_argument("--epochs", type=int, default=50, help="Number of training epochs (default: 50)")
    args = parser.parse_args()
    if args.mode == "train":
        run_train_mode(epochs=args.epochs)
    else:
        run_interactive_mode()


if __name__ == "__main__":
    main()
